{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improved_contrastive_divergence_v5.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimul/Riset-EBM/blob/main/improved_contrastive_divergence_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYwSXttcfKdw"
      },
      "source": [
        "## Mounting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB0XyqO-Fn-E"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "sample_dir = os.path.join(ROOT, 'improved_contrastive_divergence.v5')\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "os.chdir(sample_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnrqsU3yfWjH"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s81WC4PIq57N"
      },
      "source": [
        "!pip install geomloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui-z8WnHcGOK"
      },
      "source": [
        "from easydict import EasyDict\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import timeit\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from imageio import imread\n",
        "import cv2\n",
        "import scipy.spatial as ss\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.nn import Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import linalg\n",
        "from math import exp, log\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "from autograd.numpy import sqrt, sin, cos, exp, pi, prod\n",
        "from autograd.numpy.random import normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhmUyu8ZDhFW"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBe5oB41farJ"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvp4vS89cMLe"
      },
      "source": [
        "flags = EasyDict()\n",
        "\n",
        "# Configurations for distributed training\n",
        "flags['slurm'] = False # whether we are on slurm\n",
        "flags['repel_im'] = True # maximize entropy by repeling images from each other\n",
        "flags['hmc'] = False # use the hamiltonian monte carlo sampler\n",
        "flags['sampler'] = 'csgld' # use the adaptively precondition SGLD sampler\n",
        "flags['square_energy'] = False # make the energy square\n",
        "flags['alias'] = False # make the energy square\n",
        "flags['cpu'] = torch.device(\"cpu\")\n",
        "flags['gpu'] = torch.device(\"cuda:0\")\n",
        "\n",
        "flags['dataset'] = 'mnist' # cifar10 or celeba\n",
        "flags['batch_size'] = 128 #128 # batch size during training\n",
        "flags['multiscale'] = False # A multiscale EBM\n",
        "flags['self_attn'] = True #Use self attention in models\n",
        "flags['sigmoid'] = False # Apply sigmoid on energy (can improve the stability)\n",
        "flags['anneal'] = False # Decrease noise over Langevin steps\n",
        "flags['data_workers'] = 4 # Number of different data workers to load data in parallel\n",
        "flags['buffer_size'] = 10000 # Size of inputs\n",
        "\n",
        "# General Experiment Settings\n",
        "flags['exp'] = 'default' #name of experiments\n",
        "flags['log_interval'] = 50 #log outputs every so many batches\n",
        "flags['save_interval'] = 500 # save outputs every so many batches\n",
        "flags['test_interval'] = 500 # evaluate outputs every so many batches\n",
        "flags['resume_iter'] = 0 #iteration to resume training from\n",
        "flags['train'] = True # whether to train or test\n",
        "flags['transform'] = True # apply data augmentation when sampling from the replay buffer\n",
        "flags['kl'] = True # apply a KL term to loss\n",
        "flags['entropy'] = 'kl' \n",
        "flags['cuda'] = True # move device on cuda\n",
        "flags['epoch_num'] = 10 # Number of Epochs to train on\n",
        "flags['ensembles'] = 1 #Number of ensembles to train models with\n",
        "flags['lr'] = 2e-4 #Learning for training\n",
        "flags['kl_coeff'] = 1.0 #coefficient for kl\n",
        "\n",
        "# EBM Specific Experiments Settings\n",
        "flags['objective'] = 'cd' #use the cd objective\n",
        "\n",
        "# Setting for MCMC sampling\n",
        "flags['num_steps'] = 40 # Steps of gradient descent for training\n",
        "flags['step_lr'] = 10.0 # Size of steps for gradient descent\n",
        "flags['replay_batch'] = True # Use MCMC chains initialized from a replay buffer.\n",
        "flags['reservoir'] = True # Use a reservoir of past entires\n",
        "flags['noise_scale'] = 1. # Relative amount of noise for MCMC\n",
        "flags['init_noise'] = 0.1\n",
        "flags['momentum'] = 0.9\n",
        "flags['eps'] = 1e-6\n",
        "flags['step_size'] = 10\n",
        "\n",
        "# Architecture Settings\n",
        "flags['filter_dim'] = 64 #64 #number of filters for conv nets\n",
        "flags['im_size'] = 32 #32 #size of images\n",
        "flags['spec_norm'] = False #Whether to use spectral normalization on weights\n",
        "flags['norm'] = True #Use group norm in models norm in models\n",
        "\n",
        "# Conditional settings\n",
        "flags['cond'] = False #conditional generation with the model\n",
        "flags['all_step'] = False #backprop through all langevin steps\n",
        "flags['log_grad'] = False #log the gradient norm of the kl term\n",
        "flags['cond_idx'] = 0 #conditioned index\n",
        "\n",
        "DIM = 2048\n",
        "device = torch.device('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDOMH-curbuQ"
      },
      "source": [
        "writer = SummaryWriter(comment=\"_{sampler}_{entropy}_{dataset}\".format(dataset=flags.dataset, entropy=flags.entropy, sampler=flags.sampler))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5wrgilZg1Xa"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOe4s1AV4QoA"
      },
      "source": [
        "# Functions for adaptations with PyTorch:\n",
        "def to_np_array(*arrays):\n",
        "    \"\"\"Transform torch tensors/Variables into numpy arrays\"\"\"\n",
        "    array_list = []\n",
        "    for array in arrays:\n",
        "        if isinstance(array, Variable):\n",
        "            if array.is_cuda:\n",
        "                array = array.cpu()\n",
        "            array = array.data\n",
        "        if isinstance(array, torch.FloatTensor) or isinstance(array, torch.LongTensor) or isinstance(array, torch.ByteTensor) or isinstance(array, torch.cuda.FloatTensor) or isinstance(array, torch.cuda.LongTensor) or isinstance(array, torch.cuda.ByteTensor):\n",
        "            if array.is_cuda:\n",
        "                array = array.cpu()\n",
        "            array = array.numpy()\n",
        "        array_list.append(array)\n",
        "    if len(array_list) == 1:\n",
        "        array_list = array_list[0]\n",
        "    return array_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBxbSWzx4TbR"
      },
      "source": [
        "def kldiv(x, xp, k=3, base=2):\n",
        "    \"\"\" KL Divergence between p and q for x~p(x), xp~q(x)\n",
        "        x, xp should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
        "        if x is a one-dimensional scalar and we have four samples\n",
        "    \"\"\"\n",
        "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
        "    assert k <= len(xp) - 1, \"Set k smaller than num. samples - 1\"\n",
        "    assert len(x[0]) == len(xp[0]), \"Two distributions must have same dim.\"\n",
        "    x, xp = to_np_array(x, xp)\n",
        "    d = len(x[0])\n",
        "    n = len(x)\n",
        "    m = len(xp)\n",
        "    const = log(m) - log(n - 1)\n",
        "    tree = ss.cKDTree(x)\n",
        "    treep = ss.cKDTree(xp)\n",
        "    nn = [tree.query(point, k + 1, p=float('inf'))[0][k] for point in x]\n",
        "    nnp = [treep.query(point, k, p=float('inf'))[0][k - 1] for point in x]\n",
        "    return (const + d * np.mean(np.log(nnp)) - d * np.mean(np.log(nn))) / log(base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinBbPL7gMDB"
      },
      "source": [
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eB-bSnuyS9t"
      },
      "source": [
        "class WSConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(WSConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.weight\n",
        "        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n",
        "                                  keepdim=True).mean(dim=3, keepdim=True)\n",
        "        weight = weight - weight_mean\n",
        "        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n",
        "        weight = weight / std.expand_as(weight)\n",
        "        return F.conv2d(x, weight, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QyNlhx8zJrw"
      },
      "source": [
        "def compress_x_mod(x_mod):\n",
        "    x_mod = (255 * np.clip(x_mod, 0, 1)).astype(np.uint8)\n",
        "    return x_mod\n",
        "\n",
        "\n",
        "def decompress_x_mod(x_mod):\n",
        "    x_mod = x_mod / 256  + \\\n",
        "        np.random.uniform(0, 1 / 256, x_mod.shape)\n",
        "    return x_mod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgQCQlI_zOcX"
      },
      "source": [
        "def ema_model(models, models_ema, mu=0.99):\n",
        "    for model, model_ema in zip(models, models_ema):\n",
        "        for param, param_ema in zip(model.parameters(), model_ema.parameters()):\n",
        "            param_ema.data[:] = mu * param_ema.data + (1 - mu) * param.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apumq9FJMqxU"
      },
      "source": [
        "## Downsample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt9kHpSkMsMa"
      },
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)), int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]\n",
        "        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride-1)/2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        if(self.filt_size==1):\n",
        "            a = np.array([1.,])\n",
        "        elif(self.filt_size==2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size==3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size==4):\n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size==5):\n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size==6):\n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size==7):\n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a[:,None]*a[None,:])\n",
        "        filt = filt/torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))\n",
        "\n",
        "        self.pad = get_pad_layer(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size==1):\n",
        "            if(self.pad_off==0):\n",
        "                return inp[:,:,::self.stride,::self.stride]\n",
        "            else:\n",
        "                return self.pad(inp)[:,:,::self.stride,::self.stride]\n",
        "        else:\n",
        "            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "def get_pad_layer(pad_type):\n",
        "    if(pad_type in ['refl','reflect']):\n",
        "        PadLayer = nn.ReflectionPad2d\n",
        "    elif(pad_type in ['repl','replicate']):\n",
        "        PadLayer = nn.ReplicationPad2d\n",
        "    elif(pad_type=='zero'):\n",
        "        PadLayer = nn.ZeroPad2d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized'%pad_type)\n",
        "    return PadLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsPJJm_gJy9"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hM3ARE-gaKQ"
      },
      "source": [
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature\n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N) \n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        return out,attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1kb8z4rDwar"
      },
      "source": [
        "class CondResBlock(nn.Module):\n",
        "    def __init__(self, args, downsample=True, rescale=True, filters=64, latent_dim=64, im_size=64, classes=512, norm=True, spec_norm=False):\n",
        "        super(CondResBlock, self).__init__()\n",
        "\n",
        "        self.filters = filters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.im_size = im_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if filters <= 128:\n",
        "            self.bn1 = nn.InstanceNorm2d(filters, affine=True)\n",
        "        else:\n",
        "            self.bn1 = nn.GroupNorm(32, filters)\n",
        "\n",
        "        if not norm:\n",
        "            self.bn1 = None\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        if spec_norm:\n",
        "            self.conv1 = spectral_norm(nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1))\n",
        "        else:\n",
        "            self.conv1 = WSConv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if filters <= 128:\n",
        "            self.bn2 = nn.InstanceNorm2d(filters, affine=True)\n",
        "        else:\n",
        "            self.bn2 = nn.GroupNorm(32, filters, affine=True)\n",
        "\n",
        "        if not norm:\n",
        "            self.bn2 = None\n",
        "\n",
        "        if spec_norm:\n",
        "            self.conv2 = spectral_norm(nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1))\n",
        "        else:\n",
        "            self.conv2 = WSConv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.dropout = Dropout(0.2)\n",
        "\n",
        "        # Upscale to an mask of image\n",
        "        self.latent_map = nn.Linear(classes, 2*filters)\n",
        "        self.latent_map_2 = nn.Linear(classes, 2*filters)\n",
        "\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.act = swish\n",
        "\n",
        "        # Upscale to mask of image\n",
        "        if downsample:\n",
        "            if rescale:\n",
        "                self.conv_downsample = nn.Conv2d(filters, 2 * filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "                if args.alias:\n",
        "                    self.avg_pool = Downsample(channels=2*filters)\n",
        "                else:\n",
        "                    self.avg_pool = nn.AvgPool2d(3, stride=2, padding=1)\n",
        "            else:\n",
        "                self.conv_downsample = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "                if args.alias:\n",
        "                    self.avg_pool = Downsample(channels=filters)\n",
        "                else:\n",
        "                    self.avg_pool = nn.AvgPool2d(3, stride=2, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_orig = x\n",
        "\n",
        "        if y is not None:\n",
        "            latent_map = self.latent_map(y).view(-1, 2*self.filters, 1, 1)\n",
        "\n",
        "            gain = latent_map[:, :self.filters]\n",
        "            bias = latent_map[:, self.filters:]\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        if self.bn1 is not None:\n",
        "            x = self.bn1(x)\n",
        "\n",
        "        if y is not None:\n",
        "            x = gain * x + bias\n",
        "\n",
        "        x = self.act(x)\n",
        "\n",
        "        if y is not None:\n",
        "            latent_map = self.latent_map_2(y).view(-1, 2*self.filters, 1, 1)\n",
        "            gain = latent_map[:, :self.filters]\n",
        "            bias = latent_map[:, self.filters:]\n",
        "\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.bn2 is not None:\n",
        "            x = self.bn2(x)\n",
        "\n",
        "        if y is not None:\n",
        "            x = gain * x + bias\n",
        "\n",
        "        x = self.act(x)\n",
        "\n",
        "        x_out = x\n",
        "\n",
        "        if self.downsample:\n",
        "            x_out = self.conv_downsample(x_out)\n",
        "            x_out = self.act(self.avg_pool(x_out))\n",
        "\n",
        "        return x_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CG8SD8lfjay"
      },
      "source": [
        "## MNIST Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H34FfHFUcW-m"
      },
      "source": [
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.act = swish\n",
        "        # self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "        self.args = args\n",
        "        self.filter_dim = args.filter_dim\n",
        "        self.init_main_model()\n",
        "        self.init_label_map()\n",
        "        self.filter_dim = args.filter_dim\n",
        "\n",
        "        # self.act = self.relu\n",
        "        self.cond = args.cond\n",
        "        self.sigmoid = args.sigmoid\n",
        "\n",
        "\n",
        "    def init_main_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = self.filter_dim\n",
        "        im_size = 28\n",
        "        self.conv1 = nn.Conv2d(1, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.res1 = CondResBlock(args, filters=filter_dim, latent_dim=1, im_size=im_size)\n",
        "        self.res2 = CondResBlock(args, filters=2*filter_dim, latent_dim=1, im_size=im_size)\n",
        "\n",
        "        self.res3 = CondResBlock(args, filters=4*filter_dim, latent_dim=1, im_size=im_size)\n",
        "        self.energy_map = nn.Linear(filter_dim*8, 1)\n",
        "\n",
        "\n",
        "    def init_label_map(self):\n",
        "        args = self.args\n",
        "\n",
        "        self.map_fc1 = nn.Linear(10, 256)\n",
        "        self.map_fc2 = nn.Linear(256, 256)\n",
        "\n",
        "    def main_model(self, x, latent):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.res1(x, latent)\n",
        "        x = self.res2(x, latent)\n",
        "        x = self.res3(x, latent)\n",
        "        x = self.act(x)\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "        energy = self.energy_map(x)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def label_map(self, latent):\n",
        "        x = self.act(self.map_fc1(latent))\n",
        "        x = self.map_fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        args = self.args\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        if self.cond:\n",
        "            latent = self.label_map(latent)\n",
        "        else:\n",
        "            latent = None\n",
        "\n",
        "        energy = self.main_model(x, latent)\n",
        "\n",
        "        return energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLImgrK6UD3F"
      },
      "source": [
        "## Standard CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ3Rsf2jUFqo"
      },
      "source": [
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StandardCNN, self).__init__()\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(3, 64, 3, 1, 1))\n",
        "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(64, 64, 4, 2, 1))\n",
        "\n",
        "        self.conv3 = nn.utils.spectral_norm(nn.Conv2d(64, 128, 3, 1, 1))\n",
        "        self.conv4 = nn.utils.spectral_norm(nn.Conv2d(128, 128, 4, 2, 1))\n",
        "\n",
        "        self.conv5 = nn.utils.spectral_norm(nn.Conv2d(128, 256, 3, 1, 1))\n",
        "        self.conv6 = nn.utils.spectral_norm(nn.Conv2d(256, 256, 4, 2, 1))\n",
        "\n",
        "        self.conv7 = nn.utils.spectral_norm(nn.Conv2d(256, 512, 3, 1, 1))\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.act = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.dense = nn.utils.spectral_norm(nn.Linear(512 * 4 * 4, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.act(self.conv2(x))\n",
        "        # x = self.pool(x)\n",
        "        x = self.act(self.conv3(x))\n",
        "        x = self.act(self.conv4(x))\n",
        "        # x = self.pool(x)\n",
        "        x = self.act(self.conv5(x))\n",
        "        x = self.act(self.conv6(x))\n",
        "        # x = self.pool(x)\n",
        "        x = self.act(self.conv7(x))\n",
        "\n",
        "        x = self.dense(x.view(x.shape[0], -1))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MI6_NLOfmtx"
      },
      "source": [
        "## CelebA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p2DmIlkfpIQ"
      },
      "source": [
        "class CelebAModel(nn.Module):\n",
        "    def __init__(self, args, debug=False):\n",
        "        super(CelebAModel, self).__init__()\n",
        "        self.act = swish\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.cond = args.cond\n",
        "\n",
        "        self.args = args\n",
        "        self.init_main_model()\n",
        "\n",
        "        if args.multiscale:\n",
        "            self.init_mid_model()\n",
        "            self.init_small_model()\n",
        "\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.downsample = Downsample(channels=3)\n",
        "        self.heir_weight = nn.Parameter(torch.Tensor([1.0, 1.0, 1.0]))\n",
        "        self.debug = debug\n",
        "\n",
        "    def init_main_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, filter_dim // 2, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.res_1a = CondResBlock(args, filters=filter_dim // 2, latent_dim=latent_dim, im_size=im_size, downsample=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.res_4a = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "        self.res_4b = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2, norm=args.norm, spec_norm=args.spec_norm)\n",
        "\n",
        "        self.self_attn = Self_Attn(4 * filter_dim, self.act)\n",
        "\n",
        "        self.energy_map = nn.Linear(filter_dim*8, 1)\n",
        "\n",
        "    def init_mid_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.mid_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.mid_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.mid_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, classes=2)\n",
        "\n",
        "        self.mid_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.mid_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2)\n",
        "\n",
        "        self.mid_res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, classes=2)\n",
        "        self.mid_res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2)\n",
        "\n",
        "        self.mid_energy_map = nn.Linear(filter_dim*4, 1)\n",
        "        self.avg_pool = Downsample(channels=3)\n",
        "\n",
        "    def init_small_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.small_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.small_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.small_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, classes=2)\n",
        "\n",
        "        self.small_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=True, rescale=False, classes=2)\n",
        "        self.small_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, classes=2)\n",
        "\n",
        "        self.small_energy_map = nn.Linear(filter_dim*2, 1)\n",
        "\n",
        "    def main_model(self, x, latent):\n",
        "        x = self.act(self.conv1(x))\n",
        "\n",
        "        x = self.res_1a(x, latent)\n",
        "        x = self.res_1b(x, latent)\n",
        "\n",
        "        x = self.res_2a(x, latent)\n",
        "        x = self.res_2b(x, latent)\n",
        "\n",
        "\n",
        "        x = self.res_3a(x, latent)\n",
        "        x = self.res_3b(x, latent)\n",
        "\n",
        "        if self.args.self_attn:\n",
        "            x, _ = self.self_attn(x)\n",
        "\n",
        "        x = self.res_4a(x, latent)\n",
        "        x = self.res_4b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def mid_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.mid_conv1(x))\n",
        "\n",
        "        x = self.mid_res_1a(x, latent)\n",
        "        x = self.mid_res_1b(x, latent)\n",
        "\n",
        "        x = self.mid_res_2a(x, latent)\n",
        "        x = self.mid_res_2b(x, latent)\n",
        "\n",
        "        x = self.mid_res_3a(x, latent)\n",
        "        x = self.mid_res_3b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.mid_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def small_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.small_conv1(x))\n",
        "\n",
        "        x = self.small_res_1a(x, latent)\n",
        "        x = self.small_res_1b(x, latent)\n",
        "\n",
        "        x = self.small_res_2a(x, latent)\n",
        "        x = self.small_res_2b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.small_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def label_map(self, latent):\n",
        "        x = self.act(self.map_fc1(latent))\n",
        "        x = self.act(self.map_fc2(x))\n",
        "        x = self.act(self.map_fc3(x))\n",
        "        x = self.act(self.map_fc4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        args = self.args\n",
        "\n",
        "        if not self.cond:\n",
        "            latent = None\n",
        "\n",
        "        energy = self.main_model(x, latent)\n",
        "\n",
        "        if args.multiscale:\n",
        "            large_energy = energy\n",
        "            mid_energy = self.mid_model(x, latent)\n",
        "            small_energy = self.small_model(x, latent)\n",
        "            energy = torch.cat([small_energy, mid_energy, large_energy], dim=-1)\n",
        "\n",
        "        return energy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1rlR1022jzH"
      },
      "source": [
        "## ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccb-o1li2loe"
      },
      "source": [
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self.act = swish\n",
        "\n",
        "        self.args = args\n",
        "        self.spec_norm = args.spec_norm\n",
        "        self.norm = args.norm\n",
        "        self.init_main_model()\n",
        "\n",
        "        if args.multiscale:\n",
        "            self.init_mid_model()\n",
        "            self.init_small_model()\n",
        "\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.downsample = Downsample(channels=3)\n",
        "\n",
        "        self.cond = args.cond\n",
        "\n",
        "    def init_main_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.res_4a = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.res_4b = CondResBlock(args, filters=4*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.self_attn = Self_Attn(2 * filter_dim, self.act)\n",
        "\n",
        "        self.energy_map = nn.Linear(filter_dim*8, 1)\n",
        "\n",
        "    def init_mid_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.mid_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.mid_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.mid_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.mid_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.mid_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.mid_res_3a = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.mid_res_3b = CondResBlock(args, filters=2*filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.mid_energy_map = nn.Linear(filter_dim*4, 1)\n",
        "        self.avg_pool = Downsample(channels=3)\n",
        "\n",
        "    def init_small_model(self):\n",
        "        args = self.args\n",
        "        filter_dim = args.filter_dim\n",
        "        latent_dim = args.filter_dim\n",
        "        im_size = args.im_size\n",
        "\n",
        "        self.small_conv1 = nn.Conv2d(3, filter_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.small_res_1a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.small_res_1b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.small_res_2a = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, downsample=False, spec_norm=self.spec_norm, norm=self.norm)\n",
        "        self.small_res_2b = CondResBlock(args, filters=filter_dim, latent_dim=latent_dim, im_size=im_size, rescale=True, spec_norm=self.spec_norm, norm=self.norm)\n",
        "\n",
        "        self.small_energy_map = nn.Linear(filter_dim*2, 1)\n",
        "\n",
        "    def main_model(self, x, latent, compute_feat=False):\n",
        "        x = self.act(self.conv1(x))\n",
        "\n",
        "        x = self.res_1a(x, latent)\n",
        "        x = self.res_1b(x, latent)\n",
        "\n",
        "        x = self.res_2a(x, latent)\n",
        "        x = self.res_2b(x, latent)\n",
        "\n",
        "        if self.args.self_attn:\n",
        "            x, _ = self.self_attn(x)\n",
        "\n",
        "        x = self.res_3a(x, latent)\n",
        "        x = self.res_3b(x, latent)\n",
        "\n",
        "        x = self.res_4a(x, latent)\n",
        "        x = self.res_4b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        if compute_feat:\n",
        "            return x\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def mid_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.mid_conv1(x))\n",
        "\n",
        "        x = self.mid_res_1a(x, latent)\n",
        "        x = self.mid_res_1b(x, latent)\n",
        "\n",
        "        x = self.mid_res_2a(x, latent)\n",
        "        x = self.mid_res_2b(x, latent)\n",
        "\n",
        "        x = self.mid_res_3a(x, latent)\n",
        "        x = self.mid_res_3b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.mid_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def small_model(self, x, latent):\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "        x = F.avg_pool2d(x, 3, stride=2, padding=1)\n",
        "\n",
        "        x = self.act(self.small_conv1(x))\n",
        "\n",
        "        x = self.small_res_1a(x, latent)\n",
        "        x = self.small_res_1b(x, latent)\n",
        "\n",
        "        x = self.small_res_2a(x, latent)\n",
        "        x = self.small_res_2b(x, latent)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = x.mean(dim=2).mean(dim=2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        energy = self.small_energy_map(x)\n",
        "\n",
        "        if self.args.square_energy:\n",
        "            energy = torch.pow(energy, 2)\n",
        "\n",
        "        if self.args.sigmoid:\n",
        "            energy = F.sigmoid(energy)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        args = self.args\n",
        "\n",
        "        if self.cond:\n",
        "            latent = self.label_map(latent)\n",
        "        else:\n",
        "            latent = None\n",
        "\n",
        "        energy = self.main_model(x, latent)\n",
        "\n",
        "        if args.multiscale:\n",
        "            large_energy = energy\n",
        "            mid_energy = self.mid_model(x, latent)\n",
        "            small_energy = self.small_model(x, latent)\n",
        "\n",
        "            # Add a seperate energy penalizing the different energies from each model\n",
        "            energy = torch.cat([small_energy, mid_energy, large_energy], dim=-1)\n",
        "\n",
        "        return energy\n",
        "\n",
        "    def compute_feat(self, x, latent):\n",
        "        return self.main_model(x, None, compute_feat=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiIqNn3yf2Lo"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kjqZHkvgCen"
      },
      "source": [
        "class GaussianBlur(object):\n",
        "\n",
        "    def __init__(self, min=0.1, max=2.0, kernel_size=9):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56N6PS_agDLK"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, transform, dataset):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "        self.gaussian_blur = GaussianBlur()\n",
        "\n",
        "        def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "            color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.4*s)\n",
        "            rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "            rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "            color_distort = transforms.Compose([\n",
        "                rnd_color_jitter,\n",
        "                rnd_gray])\n",
        "            return color_distort\n",
        "\n",
        "        color_transform = get_color_distortion()\n",
        "\n",
        "        if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "            im_size = 32\n",
        "        elif dataset == \"continual\":\n",
        "            im_size = 64\n",
        "        elif dataset == \"celebahq\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"object\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"moving_mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"imagenet\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"lsun\":\n",
        "            im_size = 128\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "        self.dataset = dataset\n",
        "        if transform:\n",
        "            if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"continual\":\n",
        "                color_transform = get_color_distortion(0.1)\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.7, 1.0)), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"celebahq\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"imagenet\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.01, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"object\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.01, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"lsun\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"mnist\":\n",
        "                self.transform = None\n",
        "            elif dataset == \"moving_mnist\":\n",
        "                self.transform = None\n",
        "            else:\n",
        "                assert False\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, ims):\n",
        "        batch_size = ims.shape[0]\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.extend(list(ims))\n",
        "        else:\n",
        "            if batch_size + self._next_idx < self._maxsize:\n",
        "                self._storage[self._next_idx:self._next_idx +\n",
        "                              batch_size] = list(ims)\n",
        "            else:\n",
        "                split_idx = self._maxsize - self._next_idx\n",
        "                self._storage[self._next_idx:] = list(ims)[:split_idx]\n",
        "                self._storage[:batch_size - split_idx] = list(ims)[split_idx:]\n",
        "        self._next_idx = (self._next_idx + ims.shape[0]) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes, no_transform=False, downsample=False):\n",
        "        ims = []\n",
        "        for i in idxes:\n",
        "            im = self._storage[i]\n",
        "\n",
        "            if self.dataset != \"mnist\":\n",
        "                if (self.transform is not None) and (not no_transform):\n",
        "                    im = im.transpose((1, 2, 0))\n",
        "                    im = np.array(self.transform(Image.fromarray(np.array(im))))\n",
        "\n",
        "                # if downsample and (self.dataset in [\"celeba\", \"object\", \"imagenet\"]):\n",
        "                #     im = im[:, ::4, ::4]\n",
        "\n",
        "            im = im * 255\n",
        "            ims.append(im)\n",
        "        return np.array(ims)\n",
        "\n",
        "    def sample(self, batch_size, no_transform=False, downsample=False):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1)\n",
        "                 for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes, no_transform=no_transform, downsample=downsample), idxes\n",
        "\n",
        "    def set_elms(self, data, idxes):\n",
        "        if len(self._storage) < self._maxsize:\n",
        "            self.add(data)\n",
        "        else:\n",
        "            for i, ix in enumerate(idxes):\n",
        "                self._storage[ix] = data[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXPbRT7wgGEW"
      },
      "source": [
        "class ReservoirBuffer(object):\n",
        "    def __init__(self, size, transform, dataset):\n",
        "        \"\"\"Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "        self.n = 0\n",
        "\n",
        "        def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "            color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.4*s)\n",
        "            rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "            rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "            color_distort = transforms.Compose([\n",
        "                rnd_color_jitter,\n",
        "                rnd_gray])\n",
        "            return color_distort\n",
        "\n",
        "        if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "            im_size = 32\n",
        "        elif dataset == \"continual\":\n",
        "            im_size = 64\n",
        "        elif dataset == \"celeba\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"object\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"moving_mnist\":\n",
        "            im_size = 28\n",
        "        elif dataset == \"imagenet\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"lsun\":\n",
        "            im_size = 128\n",
        "        elif dataset == \"stl\":\n",
        "            im_size = 48\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "        color_transform = get_color_distortion(0.5)\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if transform:\n",
        "            if dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                color_transform = get_color_distortion(1.0)\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "                # self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.03, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"continual\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"celeba\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"imagenet\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.6, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=11), transforms.ToTensor()])\n",
        "            elif dataset == \"lsun\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=5), transforms.ToTensor()])\n",
        "            elif dataset == \"stl\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.04, 1.0)), transforms.RandomHorizontalFlip(), color_transform, GaussianBlur(kernel_size=11), transforms.ToTensor()])\n",
        "            elif dataset == \"object\":\n",
        "                self.transform = transforms.Compose([transforms.RandomResizedCrop(im_size, scale=(0.08, 1.0)), transforms.RandomHorizontalFlip(), color_transform, transforms.ToTensor()])\n",
        "            elif dataset == \"mnist\":\n",
        "                self.transform = None\n",
        "            elif dataset == \"moving_mnist\":\n",
        "                self.transform = None\n",
        "            else:\n",
        "                assert False\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, ims):\n",
        "        batch_size = ims.shape[0]\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.extend(list(ims))\n",
        "            self.n = self.n + ims.shape[0]\n",
        "        else:\n",
        "            for im in ims:\n",
        "                self.n = self.n + 1\n",
        "                ix = random.randint(0, self.n - 1)\n",
        "\n",
        "                if ix < len(self._storage):\n",
        "                    self._storage[ix] = im\n",
        "\n",
        "        self._next_idx = (self._next_idx + ims.shape[0]) % self._maxsize\n",
        "\n",
        "\n",
        "    def _encode_sample(self, idxes, no_transform=False, downsample=False):\n",
        "        ims = []\n",
        "        for i in idxes:\n",
        "            im = self._storage[i]\n",
        "\n",
        "            if self.dataset != \"mnist\":\n",
        "                if (self.transform is not None) and (not no_transform):\n",
        "                    im = im.transpose((1, 2, 0))\n",
        "                    im = np.array(self.transform(Image.fromarray(im)))\n",
        "\n",
        "            im = im * 255\n",
        "\n",
        "            ims.append(im)\n",
        "        return np.array(ims)\n",
        "\n",
        "    def sample(self, batch_size, no_transform=False, downsample=False):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        idxes = [random.randint(0, len(self._storage) - 1)\n",
        "                 for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes, no_transform=no_transform, downsample=downsample), idxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzYJvCPFhE9h"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVD8UCQ-zHlh"
      },
      "source": [
        "class Mnist(Dataset):\n",
        "    def __init__(self, train=True, rescale=1.0):\n",
        "        self.data = MNIST(\n",
        "            \"data/mnist\",\n",
        "            transform=transforms.ToTensor(),\n",
        "            download=True, train=train)\n",
        "        self.labels = np.eye(10)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im, label = self.data[index]\n",
        "        label = self.labels[label]\n",
        "        im = im.squeeze()\n",
        "        im = im.numpy() / 256 * 255 + np.random.uniform(0, 1. / 256, (28, 28))\n",
        "        im = np.clip(im, 0, 1)\n",
        "        s = 28\n",
        "        im_corrupt = np.random.uniform(0, 1, (s, s, 1))\n",
        "        im = im[:, :, None]\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPUQRGHhqcey"
      },
      "source": [
        " class CelebAHQ(Dataset):\n",
        "\n",
        "    def __init__(self, cond_idx=1, filter_idx=0):\n",
        "        self.path = \"/content/data/celebAHQ/data128x128/{:05}.jpg\"\n",
        "        self.hq_labels = pd.read_csv(os.path.join(sample_dir, \"data/celebAHQ/image_list.txt\"), sep=\"\\s+\")\n",
        "        self.labels = pd.read_csv(os.path.join(sample_dir, \"data/celebAHQ/list_attr_celeba.txt\"), sep=\"\\s+\", skiprows=1)\n",
        "        self.cond_idx = cond_idx\n",
        "        self.filter_idx = filter_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.hq_labels.shape[0] \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        info = self.hq_labels.iloc[index]\n",
        "        info = self.labels.iloc[info.orig_idx]\n",
        "\n",
        "        path = self.path.format(index+1)\n",
        "        im = np.array(Image.open(path))\n",
        "        image_size = 128\n",
        "        # im = imresize(im, (image_size, image_size))\n",
        "        im = im / 256\n",
        "        im = im + np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        label = int(info.iloc[self.cond_idx])\n",
        "        if label == -1:\n",
        "            label = 0\n",
        "        label = np.eye(2)[label]\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0, 1, size=(image_size, image_size, 3))\n",
        "\n",
        "        return im_corrupt, im, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D46zFv55qZ5"
      },
      "source": [
        "class CelebADataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            FLAGS,\n",
        "            split='train',\n",
        "            augment=False,\n",
        "            noise=True,\n",
        "            rescale=1.0):\n",
        "\n",
        "        if augment:\n",
        "            transform_list = [\n",
        "                torchvision.transforms.RandomCrop(32, padding=4),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "            transform = transforms.Compose(transform_list)\n",
        "        else:\n",
        "            # transform = transforms.ToTensor()\n",
        "            transform = transforms.Compose([\n",
        "                # resize\n",
        "                transforms.Resize(32),\n",
        "                # center-crop\n",
        "                transforms.CenterCrop(32),\n",
        "                # to-tensor\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "        self.data = torchvision.datasets.CelebA(\n",
        "            \"/content/data\",\n",
        "            transform=transform,\n",
        "            split=split,\n",
        "            download=True)\n",
        "        self.one_hot_map = np.eye(10)\n",
        "        self.noise = noise\n",
        "        self.rescale = rescale\n",
        "        self.FLAGS = FLAGS\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        FLAGS = self.FLAGS\n",
        "        \n",
        "        im, label = self.data[index]\n",
        "\n",
        "        im = np.transpose(im, (1, 2, 0)).numpy()\n",
        "        image_size = 32\n",
        "        label = self.one_hot_map[label]\n",
        "\n",
        "        im = im * 255 / 256\n",
        "\n",
        "        im = im * self.rescale + \\\n",
        "            np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        # np.random.seed((index + int(time.time() * 1e7)) % 2**32)\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0.0, self.rescale, (image_size, image_size, 3))\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label\n",
        "        # return torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Inp98HlwjB6"
      },
      "source": [
        "class Cats(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            augment=False,\n",
        "            noise=True,\n",
        "            rescale=1.0):\n",
        "\n",
        "        if augment:\n",
        "            transform_list = [\n",
        "                torchvision.transforms.RandomCrop(32, padding=4),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "            transform = transforms.Compose(transform_list)\n",
        "        else:\n",
        "            # transform = transforms.ToTensor()\n",
        "            transform = transforms.Compose([\n",
        "                # resize\n",
        "                transforms.Resize(32),\n",
        "                # center-crop\n",
        "                transforms.CenterCrop(32),\n",
        "                # to-tensor\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "        self.data = torchvision.datasets.ImageFolder('/content/data/cats', transform = transform)\n",
        "        self.one_hot_map = np.eye(10)\n",
        "        self.noise = noise\n",
        "        self.rescale = rescale\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):        \n",
        "        im, label = self.data[index]\n",
        "\n",
        "        im = np.transpose(im, (1, 2, 0)).numpy()\n",
        "        image_size = 32\n",
        "        label = self.one_hot_map[label]\n",
        "\n",
        "        im = im * 255 / 256\n",
        "\n",
        "        im = im * self.rescale + \\\n",
        "            np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0.0, self.rescale, (image_size, image_size, 3))\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfhRYDURAlRC"
      },
      "source": [
        "class Cifar10(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            FLAGS,\n",
        "            train=True,\n",
        "            full=False,\n",
        "            augment=False,\n",
        "            noise=True,\n",
        "            rescale=1.0):\n",
        "\n",
        "        if augment:\n",
        "            transform_list = [\n",
        "                torchvision.transforms.RandomCrop(32, padding=4),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "\n",
        "            transform = transforms.Compose(transform_list)\n",
        "        else:\n",
        "            transform = transforms.ToTensor()\n",
        "\n",
        "        self.full = full\n",
        "        self.data = torchvision.datasets.CIFAR10(\n",
        "            \"./data/cifar10\",\n",
        "            transform=transform,\n",
        "            train=train,\n",
        "            download=True)\n",
        "        self.test_data = torchvision.datasets.CIFAR10(\n",
        "            \"./data/cifar10\",\n",
        "            transform=transform,\n",
        "            train=False,\n",
        "            download=True)\n",
        "        self.one_hot_map = np.eye(10)\n",
        "        self.noise = noise\n",
        "        self.rescale = rescale\n",
        "        self.FLAGS = FLAGS\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.full:\n",
        "            return len(self.data) + len(self.test_data)\n",
        "        else:\n",
        "            return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        FLAGS = self.FLAGS\n",
        "        if self.full:\n",
        "            if index >= len(self.data):\n",
        "                im, label = self.test_data[index - len(self.data)]\n",
        "            else:\n",
        "                im, label = self.data[index]\n",
        "        else:\n",
        "            im, label = self.data[index]\n",
        "\n",
        "        im = np.transpose(im, (1, 2, 0)).numpy()\n",
        "        image_size = 32\n",
        "        label = self.one_hot_map[label]\n",
        "\n",
        "        im = im * 255 / 256\n",
        "\n",
        "        im = im * self.rescale + \\\n",
        "            np.random.uniform(0, 1 / 256., im.shape)\n",
        "\n",
        "        # np.random.seed((index + int(time.time() * 1e7)) % 2**32)\n",
        "\n",
        "        im_corrupt = np.random.uniform(\n",
        "            0.0, self.rescale, (image_size, image_size, 3))\n",
        "\n",
        "        return torch.Tensor(im_corrupt), torch.Tensor(im), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJaPpb3VjZkw"
      },
      "source": [
        "## Sampling ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymQuOdAXkn6"
      },
      "source": [
        "def stochastic_f(energy): \n",
        "    return energy.detach().cpu().numpy() + 0.32*normal(size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZikHdadTjZ8"
      },
      "source": [
        "def gen_image_csgld(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    im_noise = torch.randn_like(im_neg).detach()\n",
        "\n",
        "    im_negs_samples = []\n",
        "\n",
        "    parts = 100\n",
        "    Gcum = np.array(range(parts, 0, -1)) * 1.0 / sum(range(parts, 0, -1))\n",
        "    J = parts - 1\n",
        "    bouncy_move = 0\n",
        "    grad_mul = 1.\n",
        "    zeta = 0.75\n",
        "    T = 1\n",
        "    decay_lr = 100.0\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        im_noise.normal_()\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            im_neg = im_neg + 0.001 * im_noise\n",
        "\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "        # print(\"energy : \", energy)\n",
        "        lower_bound, upper_bound = np.min(energy.detach().cpu().numpy()) - 1, np.max(energy.detach().cpu().numpy()) + 1\n",
        "        partition=[lower_bound, upper_bound]\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - FLAGS.step_lr * grad_mul * im_grad\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "            im_neg_kl = im_neg_kl - FLAGS.step_lr * grad_mul * im_grad[:n]\n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            im_neg = im_neg - FLAGS.step_lr * grad_mul * im_grad\n",
        "\n",
        "        print(\"\\n grad_mul: \", grad_mul)\n",
        "        div_f = (partition[1] - partition[0]) / parts\n",
        "        grad_mul = 1 + zeta * T * (np.log(Gcum[J]) - np.log(Gcum[J-1])) / div_f\n",
        "      \n",
        "        J = (min(max(int((stochastic_f(energy).mean() - partition[0]) / div_f + 1), 1), parts - 1))\n",
        "        step_size = min(decay_lr, 10./(i**0.8+100))\n",
        "        Gcum[:J] = Gcum[:J] + step_size * Gcum[J]**zeta * (-Gcum[:J])\n",
        "        Gcum[J] = Gcum[J] + step_size * Gcum[J]**zeta * (1 - Gcum[J])\n",
        "        Gcum[(J+1):] = Gcum[(J+1):] + step_size * Gcum[J]**zeta * (-Gcum[(J+1):])\n",
        "\n",
        "        if grad_mul < 0:\n",
        "            bouncy_move = bouncy_move + 1\n",
        "            print(\"\\n bouncy_move : \", bouncy_move)\n",
        "\n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "\n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6iSSMptYXKW"
      },
      "source": [
        "def gen_image_resgld(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "\n",
        "    im_noise = torch.randn_like(im_neg).detach()\n",
        "\n",
        "    T_multiply=0.9\n",
        "    T = 0.9\n",
        "    var=0.1\n",
        "    resgld_beta_high = im_neg\n",
        "    resgld_beta_low = im_neg\n",
        "    swaps = 0\n",
        "\n",
        "    noise_scale = sqrt(2e-6 * FLAGS.step_lr * T)\n",
        "\n",
        "    print(\"noise_scale : \", noise_scale)\n",
        "    print(\"noise_scale * T_multiply: \", noise_scale* T_multiply)\n",
        "\n",
        "    im_negs_samples = []\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        im_noise.normal_()\n",
        "\n",
        "        resgld_beta_low = resgld_beta_low + noise_scale * im_noise\n",
        "        resgld_beta_high = resgld_beta_high + noise_scale * T_multiply * im_noise\n",
        "\n",
        "        resgld_beta_high.requires_grad_(requires_grad=True)\n",
        "        energy_high = model.forward(resgld_beta_high, label)\n",
        "\n",
        "        resgld_beta_low.requires_grad_(requires_grad=True)\n",
        "        energy_low = model.forward(resgld_beta_low, label)\n",
        "\n",
        "        im_grad_low = torch.autograd.grad([energy_low.sum()], [resgld_beta_low])[0]\n",
        "        im_grad_high = torch.autograd.grad([energy_high.sum()], [resgld_beta_high])[0]\n",
        "      \n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = resgld_beta_low\n",
        "            resgld_beta_low = resgld_beta_low - FLAGS.step_lr * im_grad_low \n",
        "            resgld_beta_high = resgld_beta_high - FLAGS.step_lr * im_grad_high \n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "                im_neg_kl = im_neg_kl - FLAGS.step_lr * im_grad[:n]\n",
        "                im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            resgld_beta_low = resgld_beta_low - FLAGS.step_lr * im_grad_low\n",
        "            resgld_beta_high = resgld_beta_high - FLAGS.step_lr * im_grad_high * T_multiply\n",
        "\n",
        "        dT = 1 / T - 1 / (T * T_multiply)\n",
        "        swap_rate = torch.exp(dT * (energy_low - energy_high - dT * var))\n",
        "        intensity_r = 0.1\n",
        "        # print(\"swap_rate\", swap_rate)\n",
        "        swap_rate = swap_rate.mean().item()\n",
        "        print(\"swap_rate\", swap_rate)\n",
        "        random = np.random.uniform(0, 1)\n",
        "        print(\"random\", random)\n",
        "        if random < intensity_r * swap_rate:\n",
        "            resgld_beta_high, resgld_beta_low = resgld_beta_low, resgld_beta_high\n",
        "            swaps += 1\n",
        "            print(\"swaps : \", swaps)\n",
        "\n",
        "        im_neg = resgld_beta_low.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "\n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad_low.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad_low.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zs3jBihzWtN"
      },
      "source": [
        "def rescale_im(image):\n",
        "    image = np.clip(image, 0, 1)\n",
        "    return (np.clip(image * 256, 0, 255)).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIYEMoQmzfmR"
      },
      "source": [
        "def gen_image(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    im_noise = torch.randn_like(im_neg).detach()\n",
        "\n",
        "    im_negs_samples = []\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        im_noise.normal_()\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            im_neg = im_neg + 0.001 * im_noise\n",
        "\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "            im_neg_kl = im_neg_kl - FLAGS.step_lr * im_grad[:n]\n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "\n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6XQk4MmHLBo"
      },
      "source": [
        "def gen_image_cycsgld(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    im_noise = torch.randn_like(im_neg).detach()\n",
        "    # total=1000\n",
        "    # cycles=20\n",
        "    # sub_total = total / cycles\n",
        "    # T = 1e-7\n",
        "    total=1e6\n",
        "    cycles=20\n",
        "    sub_total = total / cycles\n",
        "    T = 1e-6\n",
        "    \n",
        "    im_negs_samples = []\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        im_noise.normal_()\n",
        "        iters = i\n",
        "        r_remainder = (iters % sub_total) * 1.0 / sub_total\n",
        "        cyc_lr = FLAGS.step_lr * 5 / 2 * (cos(pi * r_remainder) + 1)\n",
        "        print(\"\\ncyc_lr\", cyc_lr)\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            # im_neg = im_neg + 0.001 * im_noise\n",
        "            im_neg = im_neg + sqrt(2 * cyc_lr * T) * im_noise\n",
        "        print(\"\\nnoise_cyc_lr\", sqrt(2 * cyc_lr * T))\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - cyc_lr * im_grad\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "            im_neg_kl = im_neg_kl - cyc_lr * im_grad[:n]\n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            im_neg = im_neg - cyc_lr * im_grad\n",
        "\n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "\n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybI_XB-nXMJV"
      },
      "source": [
        "def gen_image_psgld(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    square_avg = torch.zeros_like(im_neg)\n",
        "    im_negs_samples = []\n",
        "\n",
        "    for i in range(num_steps):\n",
        "\n",
        "        avg = square_avg.sqrt().add_(FLAGS.eps)\n",
        "        im_noise = torch.normal(mean=0,std=avg)\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            im_neg = im_neg + 0.001 * im_noise\n",
        "\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        square_avg.mul_(FLAGS.momentum).addcmul_(1 - FLAGS.momentum, im_neg.data, im_neg.data)\n",
        "        \n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad / avg\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "\n",
        "            im_neg_kl = im_neg_kl - FLAGS.step_lr * im_grad[:n] \n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "\n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuYCF6tUmQ8Q"
      },
      "source": [
        "def gen_image_asgld(label, FLAGS, model, im_neg, num_steps, sample=False):\n",
        "    stepsize = 0.2\n",
        "    noise_scale = np.sqrt(stepsize * 0.01)\n",
        "    im_noise = torch.randn_like(im_neg).detach() * noise_scale\n",
        "\n",
        "    im_negs_samples = []\n",
        "    \n",
        "    # Intialize mean and variance to zero\n",
        "    mean = torch.zeros_like(im_neg.data)\n",
        "    std = torch.zeros_like(im_neg.data)\n",
        "    weight_decay = 5e-4\n",
        "    v_noise=0.001\n",
        "    momentum=0.9\n",
        "    eps=1e-6\n",
        "    for i in range(num_steps):\n",
        "        # im_noise.normal_()\n",
        "        # Getting mean,std at previous step\n",
        "        old_mean = mean.clone()\n",
        "        old_std = std.clone()\n",
        "\n",
        "        im_noise = torch.normal(mean=old_mean, std=old_std)\n",
        "        # updt = x_negative.data.add(v_noise,im_noise)\n",
        "\n",
        "        if FLAGS.anneal:\n",
        "            im_neg = im_neg + 0.001 * (num_steps - i - 1) / num_steps * im_noise\n",
        "        else:\n",
        "            im_neg = im_neg + 0.001 * im_noise\n",
        "\n",
        "        im_neg.requires_grad_(requires_grad=True)\n",
        "        energy = model.forward(im_neg, label)\n",
        "\n",
        "        if FLAGS.all_step:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg], create_graph=True)[0]\n",
        "        else:\n",
        "            im_grad = torch.autograd.grad([energy.sum()], [im_neg])[0]\n",
        "\n",
        "        # Updating mean\n",
        "        mean = mean.mul(momentum).add(im_neg)\n",
        "        \n",
        "        # Updating std\n",
        "        part_var1 = im_neg.add(-old_mean)\n",
        "        part_var2 = im_neg.add(-mean)\n",
        "        \n",
        "        new_std = torch.pow(old_std,2).mul(momentum).addcmul(1,part_var1,part_var2).add(eps)                \n",
        "        new_std = torch.pow(torch.abs_(new_std),1/2)\n",
        "        std.add_(-1,std).add_(new_std)        \n",
        "\n",
        "        if i == num_steps - 1:\n",
        "            im_neg_orig = im_neg\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "            if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"celebahq\":\n",
        "                # Save space\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"lsun\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"object\":\n",
        "                # Save space\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"mnist\":\n",
        "                n = 128\n",
        "            elif FLAGS.dataset == \"imagenet\":\n",
        "                n = 32\n",
        "            elif FLAGS.dataset == \"stl\":\n",
        "                n = 32\n",
        "\n",
        "            im_neg_kl = im_neg_orig[:n]\n",
        "            if sample:\n",
        "                pass\n",
        "            else:\n",
        "                energy = model.forward(im_neg_kl, label)\n",
        "                im_grad = torch.autograd.grad([energy.sum()], [im_neg_kl], create_graph=True)[0]\n",
        "                \n",
        "            im_neg_kl = im_neg_kl - FLAGS.step_lr * im_grad[:n]\n",
        "            im_neg_kl = torch.clamp(im_neg_kl, 0, 1)\n",
        "        else:\n",
        "            im_neg = im_neg - FLAGS.step_lr * im_grad\n",
        "\n",
        "        im_neg = im_neg.detach()\n",
        "\n",
        "        if sample:\n",
        "            im_negs_samples.append(im_neg)\n",
        "\n",
        "        im_neg = torch.clamp(im_neg, 0, 1)\n",
        "    \n",
        "    if sample:\n",
        "        return im_neg, im_neg_kl, im_negs_samples, np.abs(im_grad.detach().cpu().numpy()).mean()\n",
        "    else:\n",
        "        return im_neg, im_neg_kl, np.abs(im_grad.detach().cpu().numpy()).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZzpwMtXnCUi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-77C3ocGz-wG"
      },
      "source": [
        "def test(model, logger, dataloader):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOiNixGFlV4"
      },
      "source": [
        "def log_tensorboard(data):  \n",
        "    writer.add_scalar(\"replay buffer length\", data[\"length_replay_buffer\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"repel loss\", data[\"loss_repel\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"batch loss\", data[\"loss\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"average loss\", data[\"avg_loss\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"KL mean loss\", data[\"kl_mean\"], data[\"iter\"])\n",
        "    \n",
        "    writer.add_scalar(\"FID\", data[\"fid\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"IS mean\", data[\"is_mean\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"IS std\", data[\"is_std\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"SSIM\", data[\"ssim\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_scalar(\"positive energy mean\", data[\"e_pos\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"positive energy std\", data[\"e_pos_std\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_scalar(\"negative energy mean\", data[\"e_neg\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"negative energy std\", data[\"e_neg_std\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_scalar(\"energy different\", data[\"e_diff\"], data[\"iter\"])\n",
        "    writer.add_scalar(\"x gradient\", data[\"x_grad\"], data[\"iter\"])\n",
        "\n",
        "    writer.add_images(\"positive examples\", data[\"positive_samples\"], data[\"iter\"])\n",
        "    writer.add_images(\"negative examples\", data[\"negative_samples\"], data[\"iter\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7RcP5Je0Ag4"
      },
      "source": [
        "def train(model, optimizer, dataloader,logdir, resume_iter, FLAGS, best_inception):\n",
        "    if FLAGS.replay_batch:\n",
        "        if FLAGS.reservoir:\n",
        "            replay_buffer = ReservoirBuffer(FLAGS.buffer_size, FLAGS.transform, FLAGS.dataset)\n",
        "        else:\n",
        "            replay_buffer = ReplayBuffer(FLAGS.buffer_size, FLAGS.transform, FLAGS.dataset)\n",
        "\n",
        "    dist_sinkhorn = SamplesLoss('sinkhorn')\n",
        "    itr = resume_iter\n",
        "    im_neg = None\n",
        "    gd_steps = 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    num_steps = FLAGS.num_steps\n",
        "\n",
        "    for epoch in range(FLAGS.epoch_num):\n",
        "        print(\"epoch : \", epoch)\n",
        "        tock = time.time()\n",
        "        average_loss = 0.0\n",
        "        for data_corrupt, data, label in tqdm(dataloader):\n",
        "            label = label.float().to(FLAGS.gpu, non_blocking=True)\n",
        "            data = data.permute(0, 3, 1, 2).float().contiguous()\n",
        "            \n",
        "            # Generate samples to evaluate inception score\n",
        "            if itr % FLAGS.save_interval == 0:\n",
        "                if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (128, 32, 32, 3)))\n",
        "                    repeat = 128 // FLAGS.batch_size + 1\n",
        "                    label = torch.cat([label] * repeat, axis=0)\n",
        "                    label = label[:128]\n",
        "                elif FLAGS.dataset == \"celebahq\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (data.shape[0], 128, 128, 3)))\n",
        "                    label = label[:data.shape[0]]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"stl\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 48, 48, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"lsun\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 128, 128, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"imagenet\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 128, 128, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"object\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (32, 128, 128, 3)))\n",
        "                    label = label[:32]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                elif FLAGS.dataset == \"mnist\":\n",
        "                    data_corrupt = torch.Tensor(np.random.uniform(0.0, 1.0, (128, 28, 28, 1)))\n",
        "                    label = label[:128]\n",
        "                    data_corrupt = data_corrupt[:label.shape[0]]\n",
        "                else:\n",
        "                    assert False\n",
        "            \n",
        "            data_corrupt = torch.Tensor(data_corrupt.float()).permute(0, 3, 1, 2).float().contiguous()\n",
        "            data = data.to(FLAGS.gpu, non_blocking=True)\n",
        "            data_corrupt = data_corrupt.to(FLAGS.gpu, non_blocking=True)\n",
        "            \n",
        "            if FLAGS.replay_batch and len(replay_buffer) >= FLAGS.batch_size:\n",
        "                replay_batch, idxs = replay_buffer.sample(data_corrupt.size(0))\n",
        "                replay_batch = decompress_x_mod(replay_batch)\n",
        "                replay_mask = (\n",
        "                    np.random.uniform(\n",
        "                        0,\n",
        "                        1,\n",
        "                        data_corrupt.size(0)) > 0.001)\n",
        "                data_corrupt[replay_mask] = torch.Tensor(replay_batch[replay_mask]).to(FLAGS.gpu, non_blocking=True)\n",
        "            else:\n",
        "                idxs = None\n",
        "\n",
        "            if FLAGS.sampler == \"psgld\":\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image_psgld(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image_psgld(label, FLAGS, model, data_corrupt, num_steps)       \n",
        "            elif FLAGS.sampler == \"asgld\":\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image_asgld(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image_asgld(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            elif FLAGS.sampler == \"sgld\":\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            elif FLAGS.sampler == \"cycsgld\":\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image_cycsgld(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image_cycsgld(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            elif FLAGS.sampler == \"resgld\":\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image_resgld(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image_resgld(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            elif FLAGS.sampler == \"csgld\":\n",
        "                if itr % FLAGS.save_interval == 0:\n",
        "                    im_neg, im_neg_kl, im_samples, x_grad = gen_image_csgld(label, FLAGS, model, data_corrupt, num_steps, sample=True)\n",
        "                else:\n",
        "                    im_neg, im_neg_kl, x_grad = gen_image_csgld(label, FLAGS, model, data_corrupt, num_steps)\n",
        "            else:\n",
        "                assert False\n",
        "            \n",
        "            data_corrupt = None\n",
        "            energy_pos = model.forward(data, label[:data.size(0)])\n",
        "            energy_neg = model.forward(im_neg, label)\n",
        "            \n",
        "            if FLAGS.replay_batch and (im_neg is not None):\n",
        "                replay_buffer.add(compress_x_mod(im_neg.detach().cpu().numpy()))\n",
        "\n",
        "            loss = energy_pos.mean() - energy_neg.mean() \n",
        "            loss = loss  + (torch.pow(energy_pos, 2).mean() + torch.pow(energy_neg, 2).mean())\n",
        "\n",
        "            if FLAGS.kl:\n",
        "                model.requires_grad_(False)\n",
        "                loss_kl = model.forward(im_neg_kl, label)\n",
        "                model.requires_grad_(True)\n",
        "                loss = loss + FLAGS.kl_coeff * loss_kl.mean()\n",
        "\n",
        "                if FLAGS.repel_im:\n",
        "                    start = timeit.timeit()\n",
        "                    bs = im_neg_kl.size(0)\n",
        "\n",
        "                    if FLAGS.dataset in [\"celebahq\", \"imagenet\", \"object\", \"lsun\", \"stl\"]:\n",
        "                        im_neg_kl = im_neg_kl[:, :, :, :].contiguous()\n",
        "\n",
        "                    im_flat = torch.clamp(im_neg_kl.view(bs, -1), 0, 1)\n",
        "\n",
        "                    if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "                        if len(replay_buffer) > 1000:\n",
        "                            compare_batch, idxs = replay_buffer.sample(100, no_transform=False)\n",
        "                            compare_batch = decompress_x_mod(compare_batch)\n",
        "                            compare_batch = torch.Tensor(compare_batch).to(FLAGS.gpu, non_blocking=True)\n",
        "                            compare_flat = compare_batch.view(100, -1)\n",
        "\n",
        "                            if FLAGS.entropy == 'kl':\n",
        "                                dist_matrix = torch.norm(im_flat[:, None, :] - compare_flat[None, :, :], p=2, dim=-1)\n",
        "                                loss_repel = torch.log(dist_matrix.min(dim=1)[0]).mean()\n",
        "                                # loss_repel = kldiv(im_flat, compare_flat)\n",
        "                                loss = loss - 0.3 * loss_repel\n",
        "                            elif FLAGS.entropy == 'sinkhorn':\n",
        "                                dist_matrix = dist_sinkhorn(im_flat, compare_flat)\n",
        "                                loss_repel = torch.log(dist_matrix).sum()\n",
        "                                loss = loss - 0.03 * loss_repel\n",
        "                            else:\n",
        "                                assert False\n",
        "                                \n",
        "                                                      \n",
        "                        else:\n",
        "                            loss_repel = torch.zeros(1)\n",
        "                        \n",
        "                        # loss = loss - 0.3 * loss_repel\n",
        "                    else:\n",
        "                        if len(replay_buffer) > 1000:\n",
        "                            compare_batch, idxs = replay_buffer.sample(100, no_transform=False, downsample=True)\n",
        "                            compare_batch = decompress_x_mod(compare_batch)\n",
        "                            compare_batch = torch.Tensor(compare_batch).to(FLAGS.gpu, non_blocking=True)\n",
        "                            compare_flat = compare_batch.view(100, -1)\n",
        "                            \n",
        "                            if FLAGS.entropy == 'kl':\n",
        "                                dist_matrix = torch.norm(im_flat[:, None, :] - compare_flat[None, :, :], p=2, dim=-1)\n",
        "                                loss_repel = torch.log(dist_matrix.min(dim=1)[0]).mean()\n",
        "                                # loss_repel = kldiv(im_flat, compare_flat)\n",
        "                            elif FLAGS.entropy == 'sinkhorn':\n",
        "                                dist_matrix = dist_sinkhorn(im_flat, compare_flat)\n",
        "                                loss_repel = torch.log(dist_matrix).sum()\n",
        "                            else:\n",
        "                                assert False\n",
        "                        else:\n",
        "                            loss_repel = torch.zeros(1).to(FLAGS.gpu, non_blocking=True)\n",
        "\n",
        "                        if FLAGS.entropy == 'kl':\n",
        "                            loss = loss - 0.3 * loss_repel  \n",
        "                        elif FLAGS.entropy == 'sinkhorn':\n",
        "                            loss = loss - 0.03 * loss_repel\n",
        "                        else:\n",
        "                            assert False\n",
        "\n",
        "                    end = timeit.timeit()\n",
        "                else:\n",
        "                    loss_repel = torch.zeros(1)\n",
        "\n",
        "            else:\n",
        "                loss_kl = torch.zeros(1)\n",
        "                loss_repel = torch.zeros(1)\n",
        "\n",
        "            if FLAGS.log_grad and len(replay_buffer) > 1000:\n",
        "                loss_kl = loss_kl - 0.1 * loss_repel\n",
        "                loss_kl = loss_kl.mean()\n",
        "                loss_ml = energy_pos.mean() - energy_neg.mean()\n",
        "\n",
        "                loss_ml.backward(retain_graph=True)\n",
        "                ele = []\n",
        "\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        ele.append(torch.norm(param.grad.data))\n",
        "\n",
        "                ele = torch.stack(ele, dim=0)\n",
        "                ml_grad = torch.mean(ele)\n",
        "                model.zero_grad()\n",
        "\n",
        "                loss_kl.backward(retain_graph=True) \n",
        "                ele = []\n",
        "\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        ele.append(torch.norm(param.grad.data))\n",
        "\n",
        "                ele = torch.stack(ele, dim=0)\n",
        "                kl_grad = torch.mean(ele)\n",
        "                model.zero_grad()\n",
        "\n",
        "            else:\n",
        "                ml_grad = None\n",
        "                kl_grad = None\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ema_model(models, models_ema)\n",
        "\n",
        "            if torch.isnan(energy_pos.mean()):\n",
        "                assert False\n",
        "\n",
        "            if torch.abs(energy_pos.mean()) > 10.0:\n",
        "                assert False\n",
        "            average_loss += (loss - average_loss) / (itr + 1)\n",
        "            if itr % FLAGS.log_interval == 0:\n",
        "                tick = time.time()\n",
        "                if FLAGS.dataset == \"mnist\":\n",
        "                    IS, FID = (0, 0), 0\n",
        "                else:\n",
        "                    IS, FID = get_inception_score_and_fid(im_neg, './cats_test.npz', verbose=True)\n",
        "                \n",
        "                ssim_value = ssim(im_neg.to(FLAGS.gpu, non_blocking=True), data.to(FLAGS.gpu, non_blocking=True))\n",
        "\n",
        "                kvs = {}\n",
        "                kvs['fid'] = FID\n",
        "                kvs['is_mean'] = IS[0]\n",
        "                kvs['is_std'] = IS[1]\n",
        "                kvs['ssim'] = ssim_value\n",
        "                kvs['e_pos'] = energy_pos.mean().item()\n",
        "                kvs['e_pos_std'] = energy_pos.std().item()\n",
        "                kvs['e_neg'] = energy_neg.mean().item()\n",
        "                kvs['kl_mean'] = loss_kl.mean().item()\n",
        "                kvs['loss_repel'] = loss_repel.mean().item()\n",
        "                kvs['loss'] = loss\n",
        "                kvs['avg_loss'] = average_loss\n",
        "                kvs['e_neg_std'] = energy_neg.std().item()\n",
        "                kvs['e_diff'] = kvs['e_pos'] - kvs['e_neg']\n",
        "                # kvs['x_grad'] = np.abs(x_grad.detach().cpu().numpy()).mean()\n",
        "                kvs['x_grad'] = x_grad\n",
        "                kvs['iter'] = itr\n",
        "                # kvs['hmc_loss'] = hmc_loss.item()\n",
        "                kvs['num_steps'] = num_steps\n",
        "                # kvs['t_diff'] = tick - tock\n",
        "                kvs['positive_samples'] = data.detach()\n",
        "                kvs['negative_samples'] = im_neg.detach()\n",
        "\n",
        "                if FLAGS.replay_batch:\n",
        "                    kvs['length_replay_buffer'] = len(replay_buffer)\n",
        "\n",
        "                # if (ml_grad is not None):\n",
        "                #     kvs['kl_grad'] = kl_grad\n",
        "                #     kvs['ml_grad'] = ml_grad\n",
        "\n",
        "                log_tensorboard(kvs)\n",
        "                tock = tick\n",
        "\n",
        "            if itr % FLAGS.save_interval == 0 and (FLAGS.save_interval != 0):\n",
        "                model_path = osp.join(logdir, \"model_{}.pth\".format(itr))\n",
        "                ckpt = {'optimizer_state_dict': optimizer.state_dict(),\n",
        "                            'FLAGS': FLAGS, 'best_inception': best_inception}\n",
        "\n",
        "                for i in range(FLAGS.ensembles):\n",
        "                    ckpt['model_state_dict_{}'.format(i)] = model.state_dict()\n",
        "                    # ckpt['ema_model_state_dict_{}'.format(i)] = model.state_dict()\n",
        "\n",
        "                torch.save(ckpt, model_path)\n",
        "\n",
        "            # if itr % FLAGS.save_interval == 0 and rank_idx == 0:\n",
        "            #     im_samples = im_samples[::10]\n",
        "            #     im_samples_total = torch.stack(im_samples, dim=1).detach().cpu().permute(0, 1, 3, 4, 2).numpy()\n",
        "            #     try_im = im_neg\n",
        "            #     orig_im = data_corrupt\n",
        "            #     actual_im = rescale_im(data.detach().permute(0, 2, 3, 1).cpu().numpy())\n",
        "\n",
        "            #     orig_im = rescale_im(orig_im.detach().permute(0, 2, 3, 1).cpu().numpy())\n",
        "            #     try_im = rescale_im(try_im.detach().permute(0, 2, 3, 1).cpu().numpy()).squeeze()\n",
        "            #     im_samples_total = rescale_im(im_samples_total)\n",
        "\n",
        "            #     if rank_idx == 0:\n",
        "            #         score, std = get_inception_score(list(try_im), splits=1)\n",
        "            #         print(\"Inception score of {} with std of {}\".format(\n",
        "            #                 score, std))\n",
        "            #         # kvs = {}\n",
        "            #         # kvs['inception_score'] = score\n",
        "            #         # kvs['inception_score_std'] = std\n",
        "            #         # logger.writekvs(kvs)\n",
        "            #         writer.add_scalar(\"inception score\", score, itr)\n",
        "            #         writer.add_scalar(\"inception score std\", std, itr)\n",
        "\n",
        "            #         if score > best_inception:\n",
        "            #             model_path = osp.join(logdir, \"model_best.pth\")\n",
        "            #             torch.save(ckpt, model_path)\n",
        "            #             best_inception = score\n",
        "\n",
        "            itr += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpUlfIX10JI7"
      },
      "source": [
        "def main_single(FLAGS):\n",
        "    print(\"Values of args: \", FLAGS)\n",
        "\n",
        "    if FLAGS.dataset == \"cifar10\":\n",
        "        train_dataset = Cifar10(FLAGS)\n",
        "        # valid_dataset = Cifar10(FLAGS, split='valid', augment=False)\n",
        "        # test_dataset = Cifar10(FLAGS, split='test', augment=False)\n",
        "    elif FLAGS.dataset == \"celeba\":\n",
        "        train_dataset = CelebADataset(FLAGS)\n",
        "        # valid_dataset = CelebADataset(FLAGS, train=False, augment=False)\n",
        "        # test_dataset = CelebADataset(FLAGS, train=False, augment=False)\n",
        "    elif FLAGS.dataset == \"cats\":\n",
        "        train_dataset = Cats()\n",
        "    elif FLAGS.dataset == \"stl\":\n",
        "        train_dataset = STLDataset(FLAGS)\n",
        "        # valid_dataset = STLDataset(FLAGS, train=False)\n",
        "        # test_dataset = STLDataset(FLAGS, train=False)\n",
        "    elif FLAGS.dataset == \"object\":\n",
        "        train_dataset = ObjectDataset(FLAGS.cond_idx)\n",
        "        # valid_dataset = ObjectDataset(FLAGS.cond_idx)\n",
        "        # test_dataset = ObjectDataset(FLAGS.cond_idx)\n",
        "    elif FLAGS.dataset == \"imagenet\":\n",
        "        train_dataset = ImageNet()\n",
        "        # valid_dataset = ImageNet()\n",
        "        # test_dataset = ImageNet()\n",
        "    elif FLAGS.dataset == \"mnist\":\n",
        "        train_dataset = Mnist(train=True)\n",
        "        # valid_dataset = Mnist(train=False)\n",
        "        # test_dataset = Mnist(train=False)\n",
        "    elif FLAGS.dataset == \"celebahq\":\n",
        "        train_dataset = CelebAHQ(cond_idx=FLAGS.cond_idx)\n",
        "        # valid_dataset = CelebAHQ(cond_idx=FLAGS.cond_idx)\n",
        "        # test_dataset = CelebAHQ(cond_idx=FLAGS.cond_idx)\n",
        "    elif FLAGS.dataset == \"lsun\":\n",
        "        train_dataset = LSUNBed(cond_idx=FLAGS.cond_idx)\n",
        "        # valid_dataset = LSUNBed(cond_idx=FLAGS.cond_idx)\n",
        "        # test_dataset = LSUNBed(cond_idx=FLAGS.cond_idx)\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=FLAGS.data_workers, batch_size=FLAGS.batch_size, shuffle=True, drop_last=True)\n",
        "    # valid_dataloader = DataLoader(valid_dataset, num_workers=FLAGS.data_workers, batch_size=FLAGS.batch_size, shuffle=True, drop_last=True)\n",
        "    # test_dataloader = DataLoader(test_dataset, num_workers=FLAGS.data_workers, batch_size=FLAGS.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    logdir = osp.join(sample_dir, FLAGS.exp, FLAGS.dataset)\n",
        "\n",
        "    best_inception = 0.0\n",
        "    \n",
        "    if FLAGS.resume_iter != 0:\n",
        "        FLAGS_OLD = FLAGS\n",
        "        model_path = osp.join(logdir, \"model_{}.pth\".format(FLAGS.resume_iter))\n",
        "        checkpoint = torch.load(model_path)\n",
        "        best_inception = checkpoint['best_inception']\n",
        "        FLAGS = checkpoint['FLAGS']\n",
        "\n",
        "        FLAGS.resume_iter = FLAGS_OLD.resume_iter\n",
        "        FLAGS_OLD = None\n",
        "\n",
        "    if FLAGS.dataset in (\"cifar10\", \"celeba\", \"cats\"):\n",
        "        model_fn = ResNetModel\n",
        "    elif FLAGS.dataset == \"stl\":\n",
        "        model_fn = ResNetModel\n",
        "    elif FLAGS.dataset == \"object\":\n",
        "        model_fn = CelebAModel\n",
        "    elif FLAGS.dataset == \"mnist\":\n",
        "        model_fn = MNISTModel\n",
        "    elif FLAGS.dataset == \"celebahq\":\n",
        "        model_fn = CelebAModel\n",
        "    elif FLAGS.dataset == \"lsun\":\n",
        "        model_fn = CelebAModel\n",
        "    elif FLAGS.dataset == \"imagenet\":\n",
        "        model_fn = ImagenetModel\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    model = model_fn(FLAGS).train()\n",
        "    # models_ema = model_fn(FLAGS).train()\n",
        "\n",
        "    if FLAGS.cuda:\n",
        "        model = model.to(FLAGS.gpu)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=FLAGS.lr, betas=(0.0, 0.9), eps=1e-8)\n",
        "\n",
        "    # ema_model(models, models_ema, mu=0.0)\n",
        "\n",
        "    it = FLAGS.resume_iter\n",
        "\n",
        "    if not osp.exists(logdir):\n",
        "        os.makedirs(logdir)\n",
        "\n",
        "    checkpoint = None\n",
        "    if FLAGS.resume_iter != 0:\n",
        "        print(\"FLAGS.resume_iter:\",FLAGS.resume_iter)\n",
        "        model_path = osp.join(logdir, \"model_{}.pth\".format(FLAGS.resume_iter))\n",
        "        checkpoint = torch.load(model_path)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        for i in range(FLAGS.ensembles):\n",
        "            model.load_state_dict(checkpoint['model_state_dict_{}'.format(i)])\n",
        "            # model_ema.load_state_dict(checkpoint['ema_model_state_dict_{}'.format(i)])\n",
        " \n",
        "\n",
        "    print(\"New Values of args: \", FLAGS)\n",
        "\n",
        "    pytorch_total_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    print(\"Number of parameters for models\", pytorch_total_params)\n",
        "\n",
        "    train(model, optimizer, train_dataloader, logdir, FLAGS.resume_iter, FLAGS, best_inception)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z_Q5eDpUvZk"
      },
      "source": [
        "## Calculate FID AND IS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGz9nb0GU1Kr"
      },
      "source": [
        "try:\n",
        "    from torchvision.models.utils import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB05nk-9U1yu"
      },
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,      # First max pooling features\n",
        "        192: 1,     # Second max pooling featurs\n",
        "        768: 2,     # Pre-aux classifier features\n",
        "        2048: 3,    # Final average pooling features\n",
        "        'prob': 4,  # softmax layer\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False,\n",
        "                 use_fid_inception=True):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        # assert self.last_needed_block <= 3, \\\n",
        "        #     'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = models.inception_v3(\n",
        "                pretrained=True, init_weights=False)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        if self.last_needed_block >= 4:\n",
        "            self.fc = inception.fc\n",
        "            self.fc.bias = None\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        if self.last_needed_block >= 4:\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            # N x 2048 x 1 x 1\n",
        "            x = torch.flatten(x, 1)\n",
        "            # N x 2048\n",
        "            x = self.fc(x)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            outp.append(x)\n",
        "\n",
        "        return outp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmJ1IMNKeIK6"
      },
      "source": [
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = models.inception_v3(num_classes=1008,\n",
        "                                    aux_logits=False,\n",
        "                                    pretrained=False,\n",
        "                                    init_weights=False)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXHDhDkXeJ-K"
      },
      "source": [
        "class FIDInceptionA(models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awnv_nXLeLoz"
      },
      "source": [
        "class FIDInceptionC(models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XDa6rxleOVC"
      },
      "source": [
        "class FIDInceptionE_1(models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxOWpvp6eP9J"
      },
      "source": [
        "class FIDInceptionE_2(models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDm8u6fZisKv"
      },
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6,\n",
        "                               use_torch=False):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "\n",
        "    Stable version by Dougal J. Sutherland.\n",
        "\n",
        "    Params:\n",
        "    -- mu1   : Numpy array containing the activations of a layer of the\n",
        "               inception net (like returned by the function 'get_predictions')\n",
        "               for generated samples.\n",
        "    -- mu2   : The sample mean over activations, precalculated on an\n",
        "               representative data set.\n",
        "    -- sigma1: The covariance matrix over activations for generated samples.\n",
        "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "               representative data set.\n",
        "\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    if use_torch:\n",
        "        assert mu1.shape == mu2.shape, \\\n",
        "            'Training and test mean vectors have different lengths'\n",
        "        assert sigma1.shape == sigma2.shape, \\\n",
        "            'Training and test covariances have different dimensions'\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "        # Run 50 itrs of newton-schulz to get the matrix sqrt of\n",
        "        # sigma1 dot sigma2\n",
        "        covmean = sqrt_newton_schulz(sigma1.mm(sigma2).unsqueeze(0), 50)\n",
        "        if torch.any(torch.isnan(covmean)):\n",
        "            return float('nan')\n",
        "        covmean = covmean.squeeze()\n",
        "        out = (diff.dot(diff) +\n",
        "               torch.trace(sigma1) +\n",
        "               torch.trace(sigma2) -\n",
        "               2 * torch.trace(covmean)).cpu().item()\n",
        "    else:\n",
        "        mu1 = np.atleast_1d(mu1)\n",
        "        mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "        sigma1 = np.atleast_2d(sigma1)\n",
        "        sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "        assert mu1.shape == mu2.shape, \\\n",
        "            'Training and test mean vectors have different lengths'\n",
        "        assert sigma1.shape == sigma2.shape, \\\n",
        "            'Training and test covariances have different dimensions'\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "\n",
        "        # Product might be almost singular\n",
        "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "        if not np.isfinite(covmean).all():\n",
        "            msg = ('fid calculation produces singular product; '\n",
        "                   'adding %s to diagonal of cov estimates') % eps\n",
        "            print(msg)\n",
        "            offset = np.eye(sigma1.shape[0]) * eps\n",
        "            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "        # Numerical error might give slight imaginary component\n",
        "        if np.iscomplexobj(covmean):\n",
        "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "                m = np.max(np.abs(covmean.imag))\n",
        "                raise ValueError('Imaginary component {}'.format(m))\n",
        "            covmean = covmean.real\n",
        "\n",
        "        tr_covmean = np.trace(covmean)\n",
        "\n",
        "        out = (diff.dot(diff) +\n",
        "               np.trace(sigma1) +\n",
        "               np.trace(sigma2) -\n",
        "               2 * tr_covmean)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-fgH7DbHm39"
      },
      "source": [
        "def get_inception_score_and_fid(\n",
        "        images,\n",
        "        fid_stats_path,\n",
        "        splits=10,\n",
        "        batch_size=50,\n",
        "        is_dataloader=False,\n",
        "        use_torch=False,\n",
        "        verbose=False):\n",
        "    \"\"\"Calculate Inception Score and FID.\n",
        "    For each image, only a forward propagation is required to\n",
        "    calculating features for FID and Inception Score.\n",
        "\n",
        "    Args:\n",
        "        images: List of tensor or torch.utils.data.Dataloader. The return image\n",
        "                must be float tensor of range [0, 1].\n",
        "        fid_stats_path: str, Path to pre-calculated statistic\n",
        "        splits: The number of bins of Inception Score. Default is 10.\n",
        "        batch_size: int, The batch size for calculating activations. If\n",
        "                    `images` is torch.utils.data.Dataloader, this arguments\n",
        "                    does not work.\n",
        "        use_torch: bool. The default value is False and the backend is same as\n",
        "                   official implementation, i.e., numpy. If use_torch is\n",
        "                   enableb, the backend linalg is implemented by torch, the\n",
        "                   results are not guaranteed to be consistent with numpy, but\n",
        "                   the speed can be accelerated by GPU.\n",
        "        verbose: int. Set verbose to 0 for disabling progress bar. Otherwise,\n",
        "                 the progress bar is showing when calculating activations.\n",
        "    Returns:\n",
        "        inception_score: float tuple, (mean, std)\n",
        "        fid: float\n",
        "    \"\"\"\n",
        "    if is_dataloader:\n",
        "        assert isinstance(images, DataLoader)\n",
        "        num_images = min(len(images.dataset), images.batch_size * len(images))\n",
        "        batch_size = images.batch_size\n",
        "    else:\n",
        "        num_images = len(images)\n",
        "\n",
        "    block_idx1 = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "    block_idx2 = InceptionV3.BLOCK_INDEX_BY_DIM['prob']\n",
        "    model = InceptionV3([block_idx1, block_idx2]).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if use_torch:\n",
        "        fid_acts = torch.empty((num_images, 2048)).to(device)\n",
        "        is_probs = torch.empty((num_images, 1008)).to(device)\n",
        "    else:\n",
        "        fid_acts = np.empty((num_images, 2048))\n",
        "        is_probs = np.empty((num_images, 1008))\n",
        "\n",
        "    pbar = tqdm(\n",
        "        total=num_images, dynamic_ncols=True, leave=False,\n",
        "        disable=not verbose, desc=\"get_inception_score_and_fid\")\n",
        "    looper = iter(images)\n",
        "    start = 0\n",
        "    while start < num_images:\n",
        "        # get a batch of images from iterator\n",
        "        if is_dataloader:\n",
        "            batch_images = next(looper)\n",
        "        else:\n",
        "            batch_images = images[start: start + batch_size]\n",
        "        end = start + len(batch_images)\n",
        "\n",
        "        # calculate inception feature\n",
        "        batch_images = batch_images.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch_images)\n",
        "            if use_torch:\n",
        "                fid_acts[start: end] = pred[0].view(-1, 2048)\n",
        "                is_probs[start: end] = pred[1]\n",
        "            else:\n",
        "                fid_acts[start: end] = pred[0].view(-1, 2048).cpu().numpy()\n",
        "                is_probs[start: end] = pred[1].cpu().numpy()\n",
        "        start = end\n",
        "        pbar.update(len(batch_images))\n",
        "    pbar.close()\n",
        "\n",
        "    # Inception Score\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        part = is_probs[\n",
        "            (i * is_probs.shape[0] // splits):\n",
        "            ((i + 1) * is_probs.shape[0] // splits), :]\n",
        "        if use_torch:\n",
        "            kl = part * (\n",
        "                torch.log(part) -\n",
        "                torch.log(torch.unsqueeze(torch.mean(part, 0), 0)))\n",
        "            kl = torch.mean(torch.sum(kl, 1))\n",
        "            scores.append(torch.exp(kl))\n",
        "        else:\n",
        "            kl = part * (\n",
        "                np.log(part) -\n",
        "                np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "            kl = np.mean(np.sum(kl, 1))\n",
        "            scores.append(np.exp(kl))\n",
        "    if use_torch:\n",
        "        scores = torch.stack(scores)\n",
        "        is_score = (torch.mean(scores).cpu().item(),\n",
        "                    torch.std(scores).cpu().item())\n",
        "    else:\n",
        "        is_score = (np.mean(scores), np.std(scores))\n",
        "\n",
        "    # FID Score\n",
        "    f = np.load(fid_stats_path)\n",
        "    m2, s2 = f['mu'][:], f['sigma'][:]\n",
        "    f.close()\n",
        "    if use_torch:\n",
        "        m1 = torch.mean(fid_acts, axis=0)\n",
        "        s1 = torch_cov(fid_acts, rowvar=False)\n",
        "        m2 = torch.tensor(m2).to(m1.dtype).to(device)\n",
        "        s2 = torch.tensor(s2).to(s1.dtype).to(device)\n",
        "    else:\n",
        "        m1 = np.mean(fid_acts, axis=0)\n",
        "        s1 = np.cov(fid_acts, rowvar=False)\n",
        "    fid_score = calculate_frechet_distance(m1, s1, m2, s2, use_torch=use_torch)\n",
        "\n",
        "    del fid_acts, is_probs, scores, model\n",
        "    return is_score, fid_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Zqa-iCSnP0"
      },
      "source": [
        "## SSIM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isR9NwPuSqNM"
      },
      "source": [
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOqhQQQGTKyk"
      },
      "source": [
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsZeS5gtTMXo"
      },
      "source": [
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYB51cH9TQ7l"
      },
      "source": [
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmB6PrzwOKMG"
      },
      "source": [
        "if flags.dataset == \"celebahq\":\n",
        "    !mkdir -p /content/data/celebAHQ\n",
        "    !unzip -qq '/content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence/data/celebAHQ/data128x128.zip' -d /content/data/celebAHQ\n",
        "elif flags.dataset == \"celeba\":\n",
        "    !mkdir -p /content/data\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence.v5\n",
        "    %cp -av data/celeba/ /content/data\n",
        "elif flags.dataset == \"cats\":\n",
        "    !mkdir -p /content/data\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence.v5\n",
        "    %cp -av data/cats/ /content/data\n",
        "    !unzip -qq /content/data/cats/cats-dataset.zip -d /content/data/cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftzvugDbDs-T"
      },
      "source": [
        " tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGcd_r9F0MqF"
      },
      "source": [
        "main_single(flags)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}