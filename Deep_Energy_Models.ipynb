{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Deep_Energy_Models.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimul/Riset-EBM/blob/main/Deep_Energy_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQF4H4ShifUK"
      },
      "source": [
        "# Tutorial 8: Deep Energy-Based Generative Models\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:** \n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial8/Deep_Energy_Models.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial8/Deep_Energy_Models.ipynb)  \n",
        "**Pre-trained models:** \n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial8)\n",
        "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/11ZI7x2sfCNtaZUNpe4v08YXWN870spXs?usp=sharing)  \n",
        "**Recordings:** \n",
        "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/E6PDwquBBQc)\n",
        "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/QJ94zuSQoP4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQjWcQQxMxPN"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "sample_dir = os.path.join(ROOT, 'deep-energy-models')\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "os.chdir(sample_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8LBH6piFD6P"
      },
      "source": [
        "!pip install torchmetrics[image]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dOTEEY9ifUa",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:12.407772Z",
          "iopub.execute_input": "2021-10-22T01:01:12.408513Z",
          "iopub.status.idle": "2021-10-22T01:01:15.840589Z",
          "shell.execute_reply.started": "2021-10-22T01:01:12.408415Z",
          "shell.execute_reply": "2021-10-22T01:01:15.83974Z"
        },
        "trusted": true
      },
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np \n",
        "import random\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST, CIFAR10, CelebA\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install pytorch-lightning==1.4.9\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "## Torchmetrics\n",
        "from torchmetrics import IS, FID\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10, MNIST)\n",
        "DATASET_PATH = \"/content/data\"\n",
        "DATASET = \"CIFAR10\"\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"runs\"\n",
        "CHECKPOINT_FILE = \"CELEBA.ckpt\"\n",
        "\n",
        "#Training hyperparameter\n",
        "TRAINING_STEPS = 60\n",
        "TRAINING_STEP_SIZE = 10\n",
        "MAX_EPOCHS = 20\n",
        "DIVERGENCE = 'contrastive_divergence'\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Initialize IS and FID\n",
        "inception_metrics = IS().to(device, non_blocking=True)\n",
        "fid_metrics = FID(feature=2048).to(device, non_blocking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPFHTAGraz03"
      },
      "source": [
        "if DATASET == 'CELEBA':\n",
        "    !mkdir -p /content/data\n",
        "    %cp -av '/content/drive/MyDrive/Colab Notebooks/improved_contrastive_divergence.v6/data/celeba' /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbdNh24LifUe"
      },
      "source": [
        "We also have pre-trained models that we download below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJfi1PnZifUf",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:15.842456Z",
          "iopub.execute_input": "2021-10-22T01:01:15.842666Z",
          "iopub.status.idle": "2021-10-22T01:01:15.849971Z",
          "shell.execute_reply.started": "2021-10-22T01:01:15.842642Z",
          "shell.execute_reply": "2021-10-22T01:01:15.848616Z"
        },
        "trusted": true
      },
      "source": [
        "# import urllib.request\n",
        "# from urllib.error import HTTPError\n",
        "# # Github URL where saved models are stored for this tutorial\n",
        "# base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial8/\"\n",
        "# # Files to download\n",
        "# pretrained_files = [\"MNIST.ckpt\", \"tensorboards/events.out.tfevents.MNIST\"]\n",
        "\n",
        "# # Create checkpoint path if it doesn't exist yet\n",
        "# os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# # For each file, check whether it already exists. If not, try downloading it.\n",
        "# for file_name in pretrained_files:\n",
        "#     file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "#     if \"/\" in file_name:\n",
        "#         os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "#     if not os.path.isfile(file_path):\n",
        "#         file_url = base_url + file_name\n",
        "#         print(f\"Downloading {file_url}...\")\n",
        "#         try:\n",
        "#             urllib.request.urlretrieve(file_url, file_path)\n",
        "#         except HTTPError as e:\n",
        "#             print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I92SglYxifUg"
      },
      "source": [
        "# Energy Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhP_zHuxifUp"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e15ognNaF41r"
      },
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = DATASET_PATH):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # Transformations applied on each image => make them a tensor and normalize between -1 and 1\n",
        "        self.transform = transforms.Compose(\n",
        "              [\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5,), (0.5,)),\n",
        "              ]\n",
        "          )\n",
        "        self.dims = (1, 28, 28)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \n",
        "        # Loading the training dataset. We need to split it into a training and validation part\n",
        "        if stage == \"fit\" or stage is None:\n",
        "          self.mnist_train = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "        \n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "          self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(self.mnist_train, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTU-i8raRshs"
      },
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = DATASET_PATH):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # Transformations applied on each image => make them a tensor and normalize between -1 and 1\n",
        "        self.transform = transforms.Compose(\n",
        "              [\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5,0.5,0.5,), (0.5,0.5,0.5,)),\n",
        "              ]\n",
        "          )\n",
        "        self.dims = (3, 32, 32)\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        CIFAR10(self.data_dir, train=True, download=True)\n",
        "        CIFAR10(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage = None):      \n",
        "        # Loading the training dataset. We need to split it into a training and validation part\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.cifar_train = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
        "        \n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(self.cifar_train, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(self.cifar_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1sMH8VfOfSt"
      },
      "source": [
        "class CelebADataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = DATASET_PATH):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # Transformations applied on each image => make them a tensor and normalize between -1 and 1\n",
        "        self.transform = transforms.Compose(\n",
        "              [\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5,0.5,0.5,), (0.5,0.5,0.5,)),\n",
        "              ]\n",
        "          )\n",
        "        self.dims = (3, 32, 32)\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        CelebA(self.data_dir, train=True, download=True)\n",
        "        CelebA(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage = None):      \n",
        "        # Loading the training dataset. We need to split it into a training and validation part\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.celeba_train = CelebA(self.data_dir, train=True, transform=self.transform)\n",
        "        \n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.celeba_test = CelebA(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(self.celeba_train, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(self.celeba_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srb0jYShifUq"
      },
      "source": [
        "## CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeq8DO5lifUr",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.525089Z",
          "iopub.execute_input": "2021-10-22T01:01:23.525362Z",
          "iopub.status.idle": "2021-10-22T01:01:23.537616Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.525328Z",
          "shell.execute_reply": "2021-10-22T01:01:23.536851Z"
        },
        "trusted": true
      },
      "source": [
        "class Swish(nn.Module):    \n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
        "        super().__init__()\n",
        "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
        "        c_hid1 = hidden_features//2\n",
        "        c_hid2 = hidden_features\n",
        "        c_hid3 = hidden_features*2\n",
        "        \n",
        "        # Series of convolutions and Swish activation functions\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "                nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4), # [16x16] - Larger padding to get 32x32 image\n",
        "                Swish(),\n",
        "                nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1), #  [8x8]\n",
        "                Swish(),\n",
        "                nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1), # [4x4]\n",
        "                Swish(),\n",
        "                nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1), # [2x2]\n",
        "                Swish(),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(c_hid3*4, c_hid3),\n",
        "                Swish(),\n",
        "                nn.Linear(c_hid3, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.538887Z",
          "iopub.execute_input": "2021-10-22T01:01:23.539231Z",
          "iopub.status.idle": "2021-10-22T01:01:23.554177Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.539191Z",
          "shell.execute_reply": "2021-10-22T01:01:23.553429Z"
        },
        "trusted": true,
        "id": "rLY5DyvNLQZw"
      },
      "source": [
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
        "        super().__init__()\n",
        "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
        "        c_hid1 = hidden_features*2 #64\n",
        "        c_hid2 = hidden_features*4 #128\n",
        "        c_hid3 = hidden_features*8 #256\n",
        "        c_hid4 = hidden_features*16 #512\n",
        "\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(3, c_hid1, 3, 1, 1))\n",
        "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(c_hid1, c_hid1, 4, 2, 1))\n",
        "\n",
        "        self.conv3 = nn.utils.spectral_norm(nn.Conv2d(c_hid1, c_hid2, 3, 1, 1))\n",
        "        self.conv4 = nn.utils.spectral_norm(nn.Conv2d(c_hid2, c_hid2, 4, 2, 1))\n",
        "\n",
        "        self.conv5 = nn.utils.spectral_norm(nn.Conv2d(c_hid2, c_hid3, 3, 1, 1))\n",
        "        self.conv6 = nn.utils.spectral_norm(nn.Conv2d(c_hid3, c_hid3, 4, 2, 1))\n",
        "\n",
        "        self.conv7 = nn.utils.spectral_norm(nn.Conv2d(c_hid3, c_hid4, 3, 1, 1))\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # self.act = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.act = Swish()\n",
        "        self.dense = nn.utils.spectral_norm(nn.Linear(c_hid4 * 4 * 4, out_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.act(self.conv2(x))\n",
        "        x = self.act(self.conv3(x))\n",
        "        x = self.act(self.conv4(x))\n",
        "        x = self.act(self.conv5(x))\n",
        "        x = self.act(self.conv6(x))\n",
        "        x = self.act(self.conv7(x))\n",
        "\n",
        "        x = self.dense(x.view(x.shape[0], -1))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMgPxKeMPhfN"
      },
      "source": [
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
        "        if isinstance(kernel_size, tuple):\n",
        "            padding = (max(kernel_size) - 1) // 2\n",
        "        else:\n",
        "            padding = (kernel_size - 1) // 2\n",
        "\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.utils.spectral_norm(nn.Conv2d(\n",
        "            in_planes,\n",
        "            out_planes,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            groups=groups,\n",
        "            bias=False,\n",
        "        ))\n",
        "        self.bn_normal = nn.BatchNorm2d(out_planes)\n",
        "        self.bn_adversial = nn.BatchNorm2d(out_planes)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, adversial=False):\n",
        "        x = self.conv(x)\n",
        "        if adversial:\n",
        "            x = self.bn_adversial(x)\n",
        "        else:\n",
        "            x = self.bn_normal(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class StandardBNCNN(nn.Module):\n",
        "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
        "        super().__init__()\n",
        "        c_hid1 = hidden_features*2 #64\n",
        "        c_hid2 = hidden_features*4 #128\n",
        "        c_hid3 = hidden_features*8 #256\n",
        "        c_hid4 = hidden_features*16 #512\n",
        "\n",
        "        self.conv1 = ConvBNReLU(3, c_hid1)\n",
        "        self.conv2 = ConvBNReLU(c_hid1, c_hid1)\n",
        "        self.conv3 = ConvBNReLU(c_hid1, c_hid2)\n",
        "        self.conv4 = ConvBNReLU(c_hid2, c_hid2)\n",
        "        self.conv5 = ConvBNReLU(c_hid2, c_hid3)\n",
        "        self.conv6 = ConvBNReLU(c_hid3, c_hid3)\n",
        "        self.conv7 = ConvBNReLU(c_hid3, c_hid4)\n",
        "        self.conv8 = ConvBNReLU(c_hid4, c_hid4)\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.utils.spectral_norm(nn.Linear(c_hid4, out_dim))\n",
        "\n",
        "    def forward(self, x, adversial=False):\n",
        "        x = self.conv1(x, adversial)\n",
        "        x = self.conv2(x, adversial)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv3(x, adversial)\n",
        "        x = self.conv4(x, adversial)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv5(x, adversial)\n",
        "        x = self.conv6(x, adversial)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv7(x, adversial)\n",
        "        x = self.conv8(x, adversial)\n",
        "        # (B, 512, 4, 4)\n",
        "\n",
        "        x = self.avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVR_qkfCQPeG"
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, n_class=None, downsample=False):\n",
        "        super().__init__()\n",
        "        #add spectral normalization to all layers of the model\n",
        "        self.conv1 = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(\n",
        "                in_channel,\n",
        "                out_channel,\n",
        "                3,\n",
        "                padding=1,\n",
        "                bias=False if n_class is not None else True,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(\n",
        "                out_channel,\n",
        "                out_channel,\n",
        "                3,\n",
        "                padding=1,\n",
        "                bias=False if n_class is not None else True,\n",
        "            ), std=1e-10, bound=True\n",
        "        )\n",
        "\n",
        "        self.class_embed = None\n",
        "\n",
        "        if n_class is not None:\n",
        "            class_embed = nn.Embedding(n_class, out_channel * 2 * 2)\n",
        "            class_embed.weight.data[:, : out_channel * 2] = 1\n",
        "            class_embed.weight.data[:, out_channel * 2 :] = 0\n",
        "\n",
        "            self.class_embed = class_embed\n",
        "\n",
        "        self.skip = None\n",
        "\n",
        "        if in_channel != out_channel or downsample:\n",
        "            self.skip = nn.Sequential(\n",
        "                spectral_norm(nn.Conv2d(in_channel, out_channel, 1, bias=False))\n",
        "            )\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, input, class_id=None):\n",
        "        out = input\n",
        "\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        if self.class_embed is not None:\n",
        "            embed = self.class_embed(class_id).view(input.shape[0], -1, 1, 1)\n",
        "            weight1, weight2, bias1, bias2 = embed.chunk(4, 1)\n",
        "            out = weight1 * out + bias1\n",
        "\n",
        "        out = F.leaky_relu(out, negative_slope=0.2)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        if self.class_embed is not None:\n",
        "            out = weight2 * out + bias2\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(input)\n",
        "\n",
        "        else:\n",
        "            skip = input\n",
        "\n",
        "        out = out + skip\n",
        "\n",
        "        if self.downsample:\n",
        "            out = F.avg_pool2d(out, 2)\n",
        "\n",
        "        out = F.leaky_relu(out, negative_slope=0.2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, n_class=None):\n",
        "        super().__init__()\n",
        "        #add spectral normalization to all layers of the model\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(3, 128, 3, padding=1), std=1)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                ResBlock(128, 128, n_class, downsample=True),\n",
        "                ResBlock(128, 128, n_class),\n",
        "                ResBlock(128, 256, n_class, downsample=True),\n",
        "                ResBlock(256, 256, n_class),\n",
        "                ResBlock(256, 256, n_class, downsample=True),\n",
        "                ResBlock(256, 256, n_class),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, input, class_id=None):\n",
        "        out = self.conv1(input)\n",
        "\n",
        "        out = F.leaky_relu(out, negative_slope=0.2)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            out = block(out, class_id)\n",
        "\n",
        "        out = F.relu(out)\n",
        "        out = out.view(out.shape[0], out.shape[1], -1).sum(2)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInyShyaifUt"
      },
      "source": [
        "## Sampling buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqAhrWyAifUu",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.556484Z",
          "iopub.execute_input": "2021-10-22T01:01:23.556707Z",
          "iopub.status.idle": "2021-10-22T01:01:23.576652Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.556684Z",
          "shell.execute_reply": "2021-10-22T01:01:23.575926Z"
        },
        "trusted": true
      },
      "source": [
        "class Sampler:\n",
        "\n",
        "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            model - Neural network to use for modeling E_theta\n",
        "            img_shape - Shape of the images to model\n",
        "            sample_size - Batch size of the samples\n",
        "            max_len - Maximum number of data points to keep in the buffer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.img_shape = img_shape\n",
        "        self.sample_size = sample_size\n",
        "        self.max_len = max_len\n",
        "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
        "\n",
        "    def sample_new_exmps(self, steps=60, step_size=10):\n",
        "        \"\"\"\n",
        "        Function for getting a new batch of \"fake\" images.\n",
        "        Inputs:\n",
        "            steps - Number of iterations in the MCMC algorithm\n",
        "            step_size - Learning rate nu in the algorithm above\n",
        "        \"\"\"\n",
        "        # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
        "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
        "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
        "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
        "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
        "\n",
        "        # Perform MCMC sampling\n",
        "        inp_imgs = Sampler.generate_contour_sgld_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
        "\n",
        "        # Add new images to the buffer and remove old ones if needed\n",
        "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
        "        self.examples = self.examples[:self.max_len]\n",
        "        return inp_imgs\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_sgld_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
        "        \"\"\"\n",
        "        Function for sampling images for a given model. \n",
        "        Inputs:\n",
        "            model - Neural network to use for modeling E_theta\n",
        "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
        "            steps - Number of iterations in the MCMC algorithm.\n",
        "            step_size - Learning rate nu in the algorithm above\n",
        "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
        "        \"\"\"\n",
        "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
        "        # because we are only interested in the gradients of the input. \n",
        "        is_training = model.training\n",
        "        model.eval()\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        inp_imgs.requires_grad = True\n",
        "        \n",
        "        # Enable gradient calculation if not already the case\n",
        "        had_gradients_enabled = torch.is_grad_enabled()\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
        "        # More efficient than creating a new tensor every iteration.\n",
        "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
        "        \n",
        "        # List for storing generations at each step (for later analysis)\n",
        "        imgs_per_step = []\n",
        "        \n",
        "        # Loop over K (steps)\n",
        "        for _ in range(steps):\n",
        "            # Part 1: Add noise to the input.\n",
        "            noise.normal_(0, 0.005)\n",
        "            inp_imgs.data.add_(noise.data)\n",
        "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "            # Part 2: calculate gradients for the current input.\n",
        "            out_imgs = -model(inp_imgs)\n",
        "            out_imgs.sum().backward()\n",
        "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
        "\n",
        "            # Apply gradients to our current samples\n",
        "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
        "            inp_imgs.grad.detach_()\n",
        "            inp_imgs.grad.zero_()\n",
        "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "            \n",
        "            if return_img_per_step:\n",
        "                imgs_per_step.append(inp_imgs.clone().detach())\n",
        "        \n",
        "        # Reactivate gradients for parameters for training\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        model.train(is_training)\n",
        "        \n",
        "        # Reset gradient calculation to setting before this function\n",
        "        torch.set_grad_enabled(had_gradients_enabled)\n",
        "\n",
        "        if return_img_per_step:\n",
        "            return torch.stack(imgs_per_step, dim=0)\n",
        "        else:\n",
        "            return inp_imgs\n",
        "        \n",
        "    @staticmethod\n",
        "    def generate_cyclic_sgld_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
        "        \"\"\"\n",
        "        Function for sampling images for a given model. \n",
        "        Inputs:\n",
        "            model - Neural network to use for modeling E_theta\n",
        "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
        "            steps - Number of iterations in the MCMC algorithm.\n",
        "            step_size - Learning rate nu in the algorithm above\n",
        "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
        "        \"\"\"\n",
        "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
        "        # because we are only interested in the gradients of the input. \n",
        "        is_training = model.training\n",
        "        model.eval()\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        inp_imgs.requires_grad = True\n",
        "        \n",
        "        # Enable gradient calculation if not already the case\n",
        "        had_gradients_enabled = torch.is_grad_enabled()\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
        "        # More efficient than creating a new tensor every iteration.\n",
        "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
        "        \n",
        "        # List for storing generations at each step (for later analysis)\n",
        "        imgs_per_step = []\n",
        "        \n",
        "        cycles=2\n",
        "        # Loop over K (steps)\n",
        "        for i in range(1, steps+1):            \n",
        "            # Part 1: Add noise to the input.\n",
        "            noise.normal_(0, 0.005)\n",
        "            inp_imgs.data.add_(noise.data)\n",
        "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "            # Part 2: calculate gradients for the current input.\n",
        "            out_imgs = -model(inp_imgs)\n",
        "            out_imgs.sum().backward()\n",
        "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
        "            \n",
        "            # Cyclical parameter\n",
        "            sub_total = steps / cycles\n",
        "            r_remainder = (i % sub_total) * 1.0 / sub_total\n",
        "            cyc_lr = step_size * 5 / 2 * (math.cos(math.pi * r_remainder) + 1)\n",
        "            \n",
        "            # Apply gradients to our current samples\n",
        "            inp_imgs.data.add_(-cyc_lr * inp_imgs.grad.data)\n",
        "            inp_imgs.grad.detach_()\n",
        "            inp_imgs.grad.zero_()\n",
        "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "            \n",
        "            if return_img_per_step:\n",
        "                imgs_per_step.append(inp_imgs.clone().detach())\n",
        "        \n",
        "        # Reactivate gradients for parameters for training\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        model.train(is_training)\n",
        "        \n",
        "        # Reset gradient calculation to setting before this function\n",
        "        torch.set_grad_enabled(had_gradients_enabled)\n",
        "\n",
        "        if return_img_per_step:\n",
        "            return torch.stack(imgs_per_step, dim=0)\n",
        "        else:\n",
        "            return inp_imgs\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_replica_sgld_samples(model, inp_low_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
        "        \"\"\"\n",
        "        Function for sampling images for a given model. \n",
        "        Inputs:\n",
        "            model - Neural network to use for modeling E_theta\n",
        "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
        "            steps - Number of iterations in the MCMC algorithm.\n",
        "            step_size - Learning rate nu in the algorithm above\n",
        "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
        "        \"\"\"\n",
        "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
        "        # because we are only interested in the gradients of the input. \n",
        "        is_training = model.training\n",
        "        model.eval()\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        inp_high_imgs = inp_low_imgs.clone()\n",
        "        \n",
        "        inp_low_imgs.requires_grad = True\n",
        "        inp_high_imgs.requires_grad = True\n",
        "\n",
        "        # Enable gradient calculation if not already the case\n",
        "        had_gradients_enabled = torch.is_grad_enabled()\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
        "        # More efficient than creating a new tensor every iteration.\n",
        "        noise_low = torch.randn(inp_low_imgs.shape, device=inp_low_imgs.device)\n",
        "        noise_high = torch.randn(inp_low_imgs.shape, device=inp_low_imgs.device)\n",
        "        \n",
        "        # List for storing generations at each step (for later analysis)\n",
        "        imgs_per_step = []\n",
        "        \n",
        "        #initialize resgld\n",
        "        T_multiply = 3\n",
        "        var = 0.1\n",
        "        T = 1.0\n",
        "\n",
        "        # Loop over K (steps)\n",
        "        for _ in range(steps):\n",
        "            # Part 1: Add noise to the low input.\n",
        "            noise_low.normal_(0, 0.003)\n",
        "            noise_high.normal_(0, 0.003 * T_multiply)\n",
        "            inp_low_imgs.data.add_(noise_low.data)\n",
        "            inp_high_imgs.data.add_(noise_high.data)\n",
        "            inp_low_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "            inp_high_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "            # Part 2: calculate gradients for the current low input.\n",
        "            out_low_imgs = -model(inp_low_imgs)\n",
        "            out_high_imgs = -model(inp_high_imgs)\n",
        "            out_low_imgs.sum().backward()\n",
        "            out_high_imgs.sum().backward()\n",
        "            inp_low_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
        "            inp_high_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
        "\n",
        "            # Apply gradients to our current samples\n",
        "            inp_low_imgs.data.add_(-step_size * inp_low_imgs.grad.data)\n",
        "            inp_high_imgs.data.add_(-step_size * inp_high_imgs.grad.data)\n",
        "            inp_low_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "            inp_high_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "            dT = (1 / T) - (1 / (T * T_multiply))\n",
        "            swap_rate = torch.exp(dT * (inp_low_imgs.grad.data - inp_high_imgs.grad.data - dT * var))\n",
        "            intensity_r = 0.1\n",
        "\n",
        "            inp_high_imgs.grad.detach_()\n",
        "            inp_high_imgs.grad.zero_()\n",
        "            inp_low_imgs.grad.detach_()\n",
        "            inp_low_imgs.grad.zero_()\n",
        "\n",
        "            if np.random.uniform(0, 1) < intensity_r * swap_rate.mean().item():\n",
        "                inp_high_imgs, inp_low_imgs = inp_low_imgs, inp_high_imgs\n",
        "\n",
        "            if return_img_per_step:\n",
        "                imgs_per_step.append(inp_low_imgs.clone().detach())\n",
        "            \n",
        "        \n",
        "        # Reactivate gradients for parameters for training\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        model.train(is_training)\n",
        "        \n",
        "        # Reset gradient calculation to setting before this function\n",
        "        torch.set_grad_enabled(had_gradients_enabled)\n",
        "\n",
        "        if return_img_per_step:\n",
        "            return torch.stack(imgs_per_step, dim=0)\n",
        "        else:\n",
        "            return inp_low_imgs\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_contour_sgld_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
        "        \"\"\"\n",
        "        Function for sampling images for a given model. \n",
        "        Inputs:\n",
        "            model - Neural network to use for modeling E_theta\n",
        "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
        "            steps - Number of iterations in the MCMC algorithm.\n",
        "            step_size - Learning rate nu in the algorithm above\n",
        "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
        "        \"\"\"\n",
        "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
        "        # because we are only interested in the gradients of the input. \n",
        "        is_training = model.training\n",
        "        model.eval()\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        inp_imgs.requires_grad = True\n",
        "        \n",
        "        # Enable gradient calculation if not already the case\n",
        "        had_gradients_enabled = torch.is_grad_enabled()\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
        "        # More efficient than creating a new tensor every iteration.\n",
        "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
        "        \n",
        "        # List for storing generations at each step (for later analysis)\n",
        "        imgs_per_step = []\n",
        "        \n",
        "        grad_mul = 1.0\n",
        "        parts = 100\n",
        "        J = parts - 1\n",
        "        grad_mul = 1.\n",
        "        Gcum = np.array(range(parts, 0, -1)) * 1.0 / sum(range(parts, 0, -1))\n",
        "        decay_lr = 3e-1\n",
        "        zeta = 0.75\n",
        "        T = 1.0\n",
        "        lower_bound, upper_bound = -7., 17.\n",
        "        div_f = (upper_bound - lower_bound) / parts\n",
        "\n",
        "        # Loop over K (steps)\n",
        "        for i in range(steps):\n",
        "            # Part 1: Add noise to the input.\n",
        "            noise.normal_(0, 0.005)\n",
        "            inp_imgs.data.add_(noise.data)\n",
        "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "            # Part 2: calculate gradients for the current input.\n",
        "            out_imgs = -model(inp_imgs)\n",
        "            out_imgs.sum().backward()\n",
        "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
        "\n",
        "            # lower_bound, upper_bound = torch.min(inp_imgs.grad.data).item() - 1, torch.max(inp_imgs.grad.data).item() + 1\n",
        "            # div_f = (upper_bound - lower_bound) / parts\n",
        "            grad_mul = 1 + zeta * T * (np.log(Gcum[J]) - np.log(Gcum[J-1])) / div_f\n",
        "            J = min(max(int((inp_imgs.grad.data.mean() - lower_bound) / div_f + 1), 1), parts - 1)\n",
        "\n",
        "            lr = min(decay_lr, 10./(i**0.8+100))\n",
        "\n",
        "            Gcum[:J] = Gcum[:J] + lr * Gcum[J]**zeta * (-Gcum[:J])\n",
        "            Gcum[J] = Gcum[J] + lr * Gcum[J]**zeta * (1 - Gcum[J])\n",
        "            Gcum[(J+1):] = Gcum[(J+1):] + lr * Gcum[J]**zeta * (-Gcum[(J+1):])\n",
        "            \n",
        "            # Apply gradients to our current samples\n",
        "            inp_imgs.data.add_(-step_size * grad_mul * inp_imgs.grad.data)\n",
        "            inp_imgs.grad.detach_()\n",
        "            inp_imgs.grad.zero_()\n",
        "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
        "\n",
        "            if return_img_per_step:\n",
        "                imgs_per_step.append(inp_imgs.clone().detach())\n",
        "        \n",
        "        # Reactivate gradients for parameters for training\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "        model.train(is_training)\n",
        "        \n",
        "        # Reset gradient calculation to setting before this function\n",
        "        torch.set_grad_enabled(had_gradients_enabled)\n",
        "\n",
        "        if return_img_per_step:\n",
        "            return torch.stack(imgs_per_step, dim=0)\n",
        "        else:\n",
        "            return inp_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPhgkbBEifUw"
      },
      "source": [
        "## Training algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GrNQOU2um3s"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egZPZPoQifUw",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.578073Z",
          "iopub.execute_input": "2021-10-22T01:01:23.578405Z",
          "iopub.status.idle": "2021-10-22T01:01:23.594513Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.578366Z",
          "shell.execute_reply": "2021-10-22T01:01:23.593894Z"
        },
        "trusted": true
      },
      "source": [
        "class DeepEnergyModel(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, img_shape, batch_size, divergence='contrastive_divergence', alpha=0.1, lr=1e-4, beta1=0.0, **CNN_args):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.cnn = CNNModel(**CNN_args) if img_shape[0] == 1 else StandardBNCNN(**CNN_args)\n",
        "        \n",
        "        self.sampler = Sampler(self.cnn, img_shape=img_shape, sample_size=batch_size)\n",
        "        self.example_input_array = torch.zeros(1, *img_shape)\n",
        "        self.last_real_imgs = torch.rand((batch_size,) + img_shape)\n",
        "        self.last_fake_imgs = torch.rand((batch_size,) + img_shape)\n",
        "        self.divergence = divergence\n",
        " \n",
        "    def forward(self, x):\n",
        "        z = self.cnn(x)\n",
        "        return z\n",
        "\n",
        "    def stable_exp(self, x):\n",
        "        return torch.exp(torch.clip(x, min=-5., max=5.))\n",
        "\n",
        "    def grad_exp(self, x):\n",
        "        if self.divergence == 'reverse_kl':\n",
        "            return - self.stable_exp(-x)\n",
        "        elif self.divergence == 'kl':\n",
        "            return 1 + x\n",
        "        elif self.divergence == 'pearson_x2':\n",
        "            return 2. * (self.stable_exp(x) - 1)\n",
        "        elif self.divergence == 'jensen_shannon':\n",
        "            return torch.log(torch.tensor(2.)) + x - torch.log(1 + self.stable_exp(x))\n",
        "        elif self.divergence == 'squared_hellinger':\n",
        "            return 1. - self.stable_exp(-0.5 * x)\n",
        "\n",
        "    def conjugate_grad_exp(self, x):\n",
        "        if self.divergence == 'reverse_kl':\n",
        "            return -1. + x\n",
        "        elif self.divergence == 'kl':\n",
        "            return self.stable_exp(x)\n",
        "        elif self.divergence == 'pearson_x2':\n",
        "            return self.stable_exp(2. * x) - 1\n",
        "        elif self.divergence == 'jensen_shannon':\n",
        "            return -torch.log(torch.tensor(2.)) + torch.log(1 + self.stable_exp(x))\n",
        "        elif self.divergence == 'squared_hellinger':\n",
        "            return self.stable_exp(0.5 * x) - 1\n",
        "        \n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        # Energy models can have issues with momentum as the loss surfaces changes with its parameters. \n",
        "        # Hence, we set it to 0 by default. \n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, 0.999))\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97) # Exponential decay over epochs\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # We add minimal noise to the original images to prevent the model from focusing on purely \"clean\" inputs\n",
        "        real_imgs, _ = batch\n",
        "        self.last_real_imgs = real_imgs.detach()\n",
        "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
        "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
        "        \n",
        "        # Obtain samples\n",
        "        fake_imgs = self.sampler.sample_new_exmps(steps=TRAINING_STEPS, step_size=TRAINING_STEP_SIZE)\n",
        "        self.last_fake_imgs = fake_imgs.detach()\n",
        "\n",
        "        # Predict energy score for all images\n",
        "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
        "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
        "        \n",
        "        # Calculate losses\n",
        "        reg_loss = 0.\n",
        "        cdiv_loss = 0.\n",
        "        f_div_loss = 0.\n",
        "        if self.divergence == 'contrastive_divergence':\n",
        "            reg_loss = self.hparams.alpha * (real_out ** 2 + fake_out ** 2).mean()\n",
        "            cdiv_loss = fake_out.mean() - real_out.mean()\n",
        "            loss = reg_loss + cdiv_loss\n",
        "        else:\n",
        "            f_div_loss = -(torch.mean(self.grad_exp(real_out)) + \n",
        "                                       torch.mean(self.conjugate_grad_exp(fake_out).detach() * fake_out) - \n",
        "                                       torch.mean(self.conjugate_grad_exp(fake_out)) - \n",
        "                                       torch.mean(fake_out) * torch.mean(self.conjugate_grad_exp(fake_out)).detach())\n",
        "            loss = f_div_loss \n",
        "        \n",
        "        # Logging\n",
        "        self.log('loss', loss)\n",
        "        self.log('loss_regularization', reg_loss)\n",
        "        self.log('loss_contrastive_divergence', cdiv_loss)\n",
        "        self.log('loss_f_divergence', f_div_loss)\n",
        "        self.log('metrics_avg_real', real_out.mean())\n",
        "        self.log('metrics_avg_fake', fake_out.mean())\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # For validating, we calculate the contrastive divergence between purely random images and unseen examples\n",
        "        # Note that the validation/test step of energy-based models depends on what we are interested in the model\n",
        "        real_imgs, _ = batch\n",
        "        fake_imgs = torch.rand_like(real_imgs) * 2 - 1\n",
        "        \n",
        "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
        "        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n",
        "        \n",
        "        cdiv = fake_out.mean() - real_out.mean()\n",
        "        self.log('val_contrastive_divergence', cdiv)\n",
        "        self.log('val_fake_out', fake_out.mean())\n",
        "        self.log('val_real_out', real_out.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdz1GnKifUx"
      },
      "source": [
        "### Callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UrTHoGzifUx",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.59587Z",
          "iopub.execute_input": "2021-10-22T01:01:23.596271Z",
          "iopub.status.idle": "2021-10-22T01:01:23.608055Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.596235Z",
          "shell.execute_reply": "2021-10-22T01:01:23.607339Z"
        },
        "trusted": true
      },
      "source": [
        "class GenerateCallback(pl.Callback):\n",
        "\n",
        "    def __init__(self, batch_size=8, vis_steps=8, num_steps=256, every_n_epochs=5):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size         # Number of images to generate\n",
        "        self.vis_steps = vis_steps           # Number of steps within generation to visualize\n",
        "        self.num_steps = num_steps           # Number of steps to take during generation\n",
        "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
        "\n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        # Skip for all other epochs\n",
        "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
        "            # Generate images\n",
        "            imgs_per_step = self.generate_imgs(pl_module)\n",
        "            # Plot and add to tensorboard\n",
        "            for i in range(imgs_per_step.shape[1]):\n",
        "                step_size = self.num_steps // self.vis_steps\n",
        "                imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
        "                grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1))\n",
        "                # grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True)\n",
        "                trainer.logger.experiment.add_image(f\"generation_{i}\", grid, global_step=trainer.current_epoch)\n",
        "                \n",
        "    def generate_imgs(self, pl_module):\n",
        "        pl_module.eval()\n",
        "        start_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
        "        start_imgs = start_imgs * 2 - 1\n",
        "        torch.set_grad_enabled(True)  # Tracking gradients for sampling necessary\n",
        "        imgs_per_step = Sampler.generate_contour_sgld_samples(pl_module.cnn, start_imgs, steps=self.num_steps, step_size=TRAINING_STEP_SIZE, return_img_per_step=True)\n",
        "        torch.set_grad_enabled(False)\n",
        "        pl_module.train()\n",
        "        return imgs_per_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNAxdRubifUz",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.60935Z",
          "iopub.execute_input": "2021-10-22T01:01:23.609595Z",
          "iopub.status.idle": "2021-10-22T01:01:23.61984Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.609563Z",
          "shell.execute_reply": "2021-10-22T01:01:23.619008Z"
        },
        "trusted": true
      },
      "source": [
        "class SamplerCallback(pl.Callback):\n",
        "    \n",
        "    def __init__(self, num_imgs=64, every_n_epochs=5):\n",
        "        super().__init__()\n",
        "        self.num_imgs = num_imgs             # Number of images to plot\n",
        "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
        "        \n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
        "            exmp_imgs = torch.cat(random.choices(pl_module.sampler.examples, k=self.num_imgs), dim=0)\n",
        "            grid = torchvision.utils.make_grid(exmp_imgs, nrow=8, normalize=True, range=(-1,1))\n",
        "            # grid = torchvision.utils.make_grid(exmp_imgs, nrow=8, normalize=True)\n",
        "            trainer.logger.experiment.add_image(\"sampler\", grid, global_step=trainer.current_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5kDYkMEifU1",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.622132Z",
          "iopub.execute_input": "2021-10-22T01:01:23.625873Z",
          "iopub.status.idle": "2021-10-22T01:01:23.633011Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.625837Z",
          "shell.execute_reply": "2021-10-22T01:01:23.632387Z"
        },
        "trusted": true
      },
      "source": [
        "class OutlierCallback(pl.Callback):\n",
        "    \n",
        "    def __init__(self, batch_size=1024):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        with torch.no_grad():\n",
        "            pl_module.eval()\n",
        "            rand_imgs = torch.rand((self.batch_size,) + pl_module.hparams[\"img_shape\"]).to(pl_module.device)\n",
        "            rand_imgs = rand_imgs * 2 - 1.0\n",
        "            rand_out = pl_module.cnn(rand_imgs).mean()\n",
        "            pl_module.train()\n",
        "        \n",
        "        trainer.logger.experiment.add_scalar(\"rand_out\", rand_out, global_step=trainer.current_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTfJPs4QB3Aa"
      },
      "source": [
        "class ImageMetricsCallback(pl.Callback):\n",
        "    \n",
        "    def __init__(self,  num_imgs=64, every_n_epochs=5):\n",
        "        super().__init__()\n",
        "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
        "        self.num_imgs = num_imgs   \n",
        "\n",
        "    def rescale_img(self, image):\n",
        "        image = np.clip(image, -1, 1)\n",
        "        return torch.from_numpy((np.clip(image * 256, 0, 255)).astype(np.uint8))\n",
        "    \n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        # images,_ = next(iter(train_loader))\n",
        "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
        "            real_imgs = pl_module.last_real_imgs\n",
        "            fake_imgs = pl_module.last_fake_imgs\n",
        "            \n",
        "            #calculate inception score\n",
        "            if real_imgs.shape[1] == 1:\n",
        "                real_imgs = torch.cat((real_imgs, real_imgs, real_imgs), dim=1)\n",
        "                fake_imgs = torch.cat((fake_imgs, fake_imgs, fake_imgs), dim=1)\n",
        "\n",
        "            real_imgs = self.rescale_img(real_imgs.cpu().numpy())\n",
        "            fake_imgs = self.rescale_img(fake_imgs.cpu().numpy())\n",
        "\n",
        "            real_imgs = real_imgs.to(pl_module.device)\n",
        "            fake_imgs = fake_imgs.to(pl_module.device)\n",
        "\n",
        "            inception_metrics.update(fake_imgs)\n",
        "            inception_mean, inception_std = inception_metrics.compute()\n",
        "            trainer.logger.experiment.add_scalar(\"inception_mean\", inception_mean.item(), global_step=trainer.current_epoch)\n",
        "            trainer.logger.experiment.add_scalar(\"inception_std\", inception_std.item(), global_step=trainer.current_epoch)\n",
        "\n",
        "            #calculate FID\n",
        "            fid_metrics.update(real_imgs, real=True)\n",
        "            fid_metrics.update(fake_imgs, real=False)\n",
        "            fid_val = fid_metrics.compute()\n",
        "            trainer.logger.experiment.add_scalar(\"fid_value\", fid_val.item(), global_step=trainer.current_epoch)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO8w0uAMifU6"
      },
      "source": [
        "### TensorBoard\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7TxH7oKifU7",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.649699Z",
          "iopub.status.idle": "2021-10-22T01:03:41.650424Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.650181Z",
          "shell.execute_reply": "2021-10-22T01:03:41.650207Z"
        },
        "trusted": true
      },
      "source": [
        "# Import tensorboard\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpTqOLvqifU8",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.655605Z",
          "iopub.status.idle": "2021-10-22T01:03:41.656215Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.655981Z",
          "shell.execute_reply": "2021-10-22T01:03:41.656006Z"
        },
        "trusted": true
      },
      "source": [
        "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpjYLqoIqLev"
      },
      "source": [
        "### Running the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlL1-9fxNF5d"
      },
      "source": [
        "MAX_NUM_STEPS = 256\n",
        "if DATASET == 'MNIST':\n",
        "    data_module = MNISTDataModule()\n",
        "elif DATASET == 'CIFAR10':\n",
        "    MAX_NUM_STEPS = 1024\n",
        "    data_module = CIFAR10DataModule()\n",
        "else:\n",
        "    assert False\n",
        "\n",
        "data_module.prepare_data()\n",
        "data_module.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOF5wdQrifU1",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.635968Z",
          "iopub.execute_input": "2021-10-22T01:01:23.636309Z",
          "iopub.status.idle": "2021-10-22T01:01:23.645695Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.63628Z",
          "shell.execute_reply": "2021-10-22T01:01:23.64509Z"
        },
        "trusted": true
      },
      "source": [
        "def train_model(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, DATASET, DIVERGENCE),\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
        "                         max_epochs=MAX_EPOCHS,\n",
        "                         gradient_clip_val=0.1,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor='val_contrastive_divergence'),\n",
        "                                    GenerateCallback(every_n_epochs=1),\n",
        "                                    SamplerCallback(every_n_epochs=1),\n",
        "                                    OutlierCallback(),\n",
        "                                    ImageMetricsCallback(every_n_epochs=1),\n",
        "                                    LearningRateMonitor(\"epoch\")\n",
        "                                   ],\n",
        "                        progress_bar_refresh_rate=1)\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, CHECKPOINT_FILE)\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = DeepEnergyModel.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)\n",
        "        model = DeepEnergyModel(**kwargs)\n",
        "        trainer.fit(model, data_module)\n",
        "        model = DeepEnergyModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    # No testing as we are more interested in other properties\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gltmXoLoifU2",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:01:23.648533Z",
          "iopub.execute_input": "2021-10-22T01:01:23.648918Z",
          "iopub.status.idle": "2021-10-22T01:03:41.646394Z",
          "shell.execute_reply.started": "2021-10-22T01:01:23.648885Z",
          "shell.execute_reply": "2021-10-22T01:03:41.643516Z"
        },
        "trusted": true
      },
      "source": [
        "model = train_model(divergence=DIVERGENCE,\n",
        "                    img_shape=data_module.dims, \n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    lr=1e-4,\n",
        "                    beta1=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1s9rKJSifU6"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "In the last part of the notebook, we will try to take the trained energy-based generative model, and analyse its properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HnhndbeifU-"
      },
      "source": [
        "### Image Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tefxrJpKifU-",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.661151Z",
          "iopub.status.idle": "2021-10-22T01:03:41.66188Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.661647Z",
          "shell.execute_reply": "2021-10-22T01:03:41.661671Z"
        },
        "trusted": true
      },
      "source": [
        "model.to(device)\n",
        "pl.seed_everything(43)\n",
        "callback = GenerateCallback(batch_size=8, vis_steps=8, num_steps=MAX_NUM_STEPS)\n",
        "imgs_per_step = callback.generate_imgs(model)\n",
        "imgs_per_step = imgs_per_step.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl2aQ5XIifU_"
      },
      "source": [
        "The characteristic of sampling with energy-based models is that they require the iterative MCMC algorithm. To gain an insight in how the images change over iterations, we plot a few intermediate samples in the MCMC as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I25GSxl0ifU_",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.663889Z",
          "iopub.status.idle": "2021-10-22T01:03:41.665823Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.664607Z",
          "shell.execute_reply": "2021-10-22T01:03:41.664634Z"
        },
        "trusted": true
      },
      "source": [
        "for i in range(imgs_per_step.shape[1]):\n",
        "    step_size = callback.num_steps // callback.vis_steps\n",
        "    imgs_to_plot = imgs_per_step[step_size-1::step_size,i]\n",
        "    imgs_to_plot = torch.cat([imgs_per_step[0:1,i],imgs_to_plot], dim=0)\n",
        "    grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(grid)\n",
        "    plt.xlabel(\"Generation iteration\")\n",
        "    plt.xticks([(imgs_per_step.shape[-1]+2)*(0.5+j) for j in range(callback.vis_steps+1)], \n",
        "               labels=[1] + list(range(step_size,imgs_per_step.shape[0]+1,step_size)))\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QLyJbGMifVA"
      },
      "source": [
        "### Out-of-distribution detection\n",
        "\n",
        "A very common and strong application of energy-based models is out-of-distribution detection (sometimes referred to as \"anomaly\" detection). As more and more deep learning models are applied in production and applications, a crucial aspect of these models is to know what the models don't know. Deep learning models are usually overconfident, meaning that they classify even random images sometimes with 100% probability. Clearly, this is not something that we want to see in applications. Energy-based models can help with this problem because they are trained to detect images that do not fit the training dataset distribution. Thus, in those applications, you could train an energy-based model along with the classifier, and only output predictions if the energy-based models assign a (unnormalized) probability higher than $\\delta$ to the image. You can actually combine classifiers and energy-based objectives in a single model, as proposed in this [paper](https://arxiv.org/abs/1912.03263).\n",
        "\n",
        "In this part of the analysis, we want to test the out-of-distribution capability of our energy-based model. Remember that a lower output of the model denotes a low probability. Thus, we hope to see low scores if we enter random noise to the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usMHZm2KifVA",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.668126Z",
          "iopub.status.idle": "2021-10-22T01:03:41.668734Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.668511Z",
          "shell.execute_reply": "2021-10-22T01:03:41.668534Z"
        },
        "trusted": true
      },
      "source": [
        "with torch.no_grad():\n",
        "    rand_imgs = torch.rand((128,) + model.hparams.img_shape).to(model.device)\n",
        "    rand_imgs = rand_imgs * 2 - 1.0\n",
        "    rand_out = model.cnn(rand_imgs).mean()\n",
        "    print(f\"Average score for random images: {rand_out.item():4.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbWE0KdEifVB"
      },
      "source": [
        "As we hoped, the model assigns very low probability to those noisy images. As another reference, let's look at predictions for a batch of images from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvTcCmDhifVB",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.670838Z",
          "iopub.status.idle": "2021-10-22T01:03:41.672726Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.672389Z",
          "shell.execute_reply": "2021-10-22T01:03:41.672416Z"
        },
        "trusted": true
      },
      "source": [
        "with torch.no_grad():\n",
        "    train_imgs,_ = next(iter(data_module.val_dataloader()))\n",
        "    train_imgs = train_imgs.to(model.device)\n",
        "    train_out = model.cnn(train_imgs).mean()\n",
        "    print(f\"Average score for training images: {train_out.item():4.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evPo2_ibifVC"
      },
      "source": [
        "The scores are close to 0 because of the regularization objective that was added to the training. So clearly, the model can distinguish between noise and real digits. However, what happens if we change the training images a little, and see which ones gets a very low score?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diYmGUiJifVD",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.674093Z",
          "iopub.status.idle": "2021-10-22T01:03:41.675228Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.674983Z",
          "shell.execute_reply": "2021-10-22T01:03:41.67501Z"
        },
        "trusted": true
      },
      "source": [
        "@torch.no_grad()\n",
        "def compare_images(img1, img2):\n",
        "    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n",
        "    score1, score2 = model.cnn(imgs).cpu().chunk(2, dim=0)\n",
        "    # grid = torchvision.utils.make_grid([img1.cpu(), img2.cpu()], nrow=2, normalize=True, range=(-1,1), pad_value=0.5, padding=2)\n",
        "    grid = torchvision.utils.make_grid([img1.cpu(), img2.cpu()], nrow=2, normalize=True, pad_value=0.5, padding=2)\n",
        "    grid = grid.permute(1, 2, 0)\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(grid)\n",
        "    plt.xticks([(img1.shape[2]+2)*(0.5+j) for j in range(2)],\n",
        "               labels=[\"Original image\", \"Transformed image\"])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "    print(\"Score original image: %4.2f\" % score1)\n",
        "    print(\"Score transformed image: %4.2f\" % score2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7l3tyHOifVD"
      },
      "source": [
        "We use a random test image for this. Feel free to change it to experiment with the model yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXXVdohRifVE",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.67802Z",
          "iopub.status.idle": "2021-10-22T01:03:41.67871Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.678407Z",
          "shell.execute_reply": "2021-10-22T01:03:41.678466Z"
        },
        "trusted": true
      },
      "source": [
        "test_imgs, _ = next(iter(dm.val_dataloader()))\n",
        "exmp_img = test_imgs[0].to(model.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhXS3KcIifVE"
      },
      "source": [
        "The first transformation is to add some random noise to the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig0E1bAHifVE",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.681378Z",
          "iopub.status.idle": "2021-10-22T01:03:41.682463Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.682122Z",
          "shell.execute_reply": "2021-10-22T01:03:41.682148Z"
        },
        "trusted": true
      },
      "source": [
        "img_noisy = exmp_img + torch.randn_like(exmp_img) * 0.3\n",
        "img_noisy.clamp_(min=-1.0, max=1.0)\n",
        "compare_images(exmp_img, img_noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA3Gs6afifVG"
      },
      "source": [
        "We can see that the score considerably drops. Hence, the model can detect random Gaussian noise on the image. This is also to expect as initially, the \"fake\" samples are pure noise images.\n",
        "\n",
        "Next, we flip an image and check how this influences the score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7uiM7m0ifVG",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.684318Z",
          "iopub.status.idle": "2021-10-22T01:03:41.686321Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.685977Z",
          "shell.execute_reply": "2021-10-22T01:03:41.686026Z"
        },
        "trusted": true
      },
      "source": [
        "img_flipped = exmp_img.flip(dims=(1,2))\n",
        "compare_images(exmp_img, img_flipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9K9vA4hifVK"
      },
      "source": [
        "If the digit can only be read in this way, for example, the 7, then we can see that the score drops. However, the score only drops slightly. This is likely because of the small size of our model. Keep in mind that generative modeling is a much harder task than classification, as we do not only need to distinguish between classes but learn **all** details/characteristics of the digits. With a deeper model, this could eventually be captured better (but at the cost of greater training instability).\n",
        "\n",
        "Finally, we check what happens if we reduce the digit significantly in size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4idemmqOifVM",
        "execution": {
          "iopub.status.busy": "2021-10-22T01:03:41.688898Z",
          "iopub.status.idle": "2021-10-22T01:03:41.689565Z",
          "shell.execute_reply.started": "2021-10-22T01:03:41.689318Z",
          "shell.execute_reply": "2021-10-22T01:03:41.689343Z"
        },
        "trusted": true
      },
      "source": [
        "img_tiny = torch.zeros_like(exmp_img)-1\n",
        "img_tiny[:,exmp_img.shape[1]//2:,exmp_img.shape[2]//2:] = exmp_img[:,::2,::2]\n",
        "compare_images(exmp_img, img_tiny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gswKQFwmifVM"
      },
      "source": [
        "The score again drops but not by a large margin, although digits in the MNIST dataset usually are much larger. \n",
        "\n",
        "Overall, we can conclude that our model is good for detecting Gaussian noise and smaller transformations to existing digits. Nonetheless, to obtain a very good out-of-distribution model, we would need to train deeper models and for more iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA7w9UGcifVN"
      },
      "source": [
        "### Instability\n",
        "\n",
        "Finally, we should discuss the possible instabilities of energy-based models, in particular for the example of image generation that we have implemented in this notebook. In the process of hyperparameter search for this notebook, there have been several models that diverged. Divergence in energy-based models means that the models assign a high probability to examples of the training set which is a good thing. However, at the same time, the sampling algorithm fails and only generates noise images that obtain minimal probability scores. This happens because the model has created many local maxima in which the generated noise images fall. The energy surface over which we calculate the gradients to reach data points with high probability has \"diverged\" and is not useful for our MCMC sampling.\n",
        "\n",
        "Besides finding the optimal hyperparameters, a common trick in energy-based models is to reload stable checkpoints. If we detect that the model is diverging, we stop the training, load the model from one epoch ago where it did not diverge yet. Afterward, we continue training and hope that with a different seed the model is not diverging again. Nevertheless, this should be considered as the \"last hope\" for stabilizing the models, and careful hyperparameter tuning is the better way to do so. Sensitive hyperparameters include `step_size`, `steps` and the noise standard deviation in the sampler, and the learning rate and feature dimensionality in the CNN model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwKVoQZkifVN"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have discussed energy-based models for generative modeling. The concept relies on the idea that any strictly positive function can be turned into a probability distribution by normalizing over the whole dataset. As this is not reasonable to calculate for high dimensional data like images, we train the model using contrastive divergence and sampling via MCMC. While the idea allows us to turn any neural network into an energy-based model, we have seen that there are multiple training tricks needed to stabilize the training. Furthermore, the training time of these models is relatively long as, during every training iteration, we need to sample new \"fake\" images, even with a sampling buffer. In the next lectures and assignment, we will see different generative models (e.g. VAE, GAN, NF) that allow us to do generative modeling more stably, but with the cost of more parameters."
      ]
    }
  ]
}